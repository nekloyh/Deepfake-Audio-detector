{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a81a32f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import torch\n",
    "import torchaudio.transforms as T\n",
    "import logging\n",
    "import hashlib\n",
    "from typing import List, Tuple, Optional\n",
    "from pathlib import Path\n",
    "import pyloudnorm as pyln\n",
    "import random\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a62fac2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration Section ---\n",
    "class DataConfig:\n",
    "    \"\"\"Configuration for audio data processing parameters.\"\"\"\n",
    "\n",
    "    # General audio processing settings\n",
    "    SEED = 42  # Random seed for reproducibility\n",
    "\n",
    "    SR = 16000  # Sample rate (Hz)\n",
    "    N_FFT = 2048  # FFT window size\n",
    "    HOP_LENGTH = 512  # Hop length for spectrogram\n",
    "    N_MELS = 128  # Number of Mel bands\n",
    "    FMIN = 0.0  # Minimum frequency (Hz)\n",
    "    FMAX = 8000.0  # Maximum frequency (Hz)\n",
    "\n",
    "    # Augmentation settings\n",
    "    NUM_TIME_MASKS = 2  # Number of time masks for SpecAugment\n",
    "    NUM_FREQ_MASKS = 2  # Number of frequency masks for SpecAugment\n",
    "    TIME_MASK_MAX_WIDTH = 60  # Maximum width of time mask\n",
    "    FREQ_MASK_MAX_WIDTH = 25  # Maximum width of frequency mask\n",
    "    MASK_REPLACEMENT_VALUE = -80.0  # Value for masked regions in spectrogram\n",
    "    NORM_EPSILON = 1e-6  # Small value to prevent division by zero\n",
    "    LOUDNESS_LUFS = -23.0  # Target loudness (LUFS)\n",
    "\n",
    "    # Dataset and processing options\n",
    "    USE_GLOBAL_NORMALIZATION = False  # Use global mean/std for normalization\n",
    "    USE_RANDOM_CROPPING = True  # Apply random cropping to spectrograms\n",
    "    DATASET_ROOT = \"F:\\\\Deepfake-Audio-Detector\\\\dataset\"  # Root directory for raw dataset\n",
    "    CACHE_DIR = \"F:\\\\Deepfake-Audio-Detector\\\\processed_dataset\"  # Directory for processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c48d59fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 07:33:49,308 - INFO - CUDA available. Applied CUDA seeds.\n"
     ]
    }
   ],
   "source": [
    "random.seed(DataConfig.SEED)\n",
    "np.random.seed(DataConfig.SEED)\n",
    "torch.manual_seed(DataConfig.SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(DataConfig.SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    logging.info(\"CUDA available. Applied CUDA seeds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a49a8644",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaderConfig:\n",
    "    \"\"\"Configuration for DataLoader creation\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        audio_length_seconds: float,\n",
    "        batch_size: int,\n",
    "        num_workers: int = 12,\n",
    "        apply_augmentation_to_train: bool = True,\n",
    "        apply_waveform_augmentation: bool = True,\n",
    "        limit_files: Optional[int] = None,\n",
    "        overlap_ratio: float = 0.0,\n",
    "    ):\n",
    "        self.audio_length_seconds = audio_length_seconds\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.apply_augmentation_to_train = apply_augmentation_to_train\n",
    "        self.apply_waveform_augmentation = apply_waveform_augmentation\n",
    "        self.limit_files = limit_files\n",
    "        self.overlap_ratio = overlap_ratio\n",
    "        self.max_frame_spec = int(\n",
    "            np.ceil((audio_length_seconds * DataConfig.SR) / DataConfig.HOP_LENGTH)\n",
    "        )\n",
    "        logging.info(\n",
    "            f\"DataLoaderConfig initialized with audio_length_seconds={audio_length_seconds}, \"\n",
    "            f\"max_frame_spec={self.max_frame_spec} frames, \"\n",
    "            f\"(SR={DataConfig.SR}, HOP_LENGTH={DataConfig.HOP_LENGTH})\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1c8e189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Functions ---\n",
    "def _load_and_segment_audio(\n",
    "    file_path: Optional[str],\n",
    "    sr: int = DataConfig.SR,\n",
    "    segment_length: float = 5.0,\n",
    "    overlap_ratio: float = 0.0,\n",
    ") -> List[np.ndarray]:\n",
    "    \"\"\"Load and segment audio into fixed-length parts with loudness normalization.\"\"\"\n",
    "    try:\n",
    "        y, _ = librosa.load(str(file_path), sr=sr, mono=True)\n",
    "        \n",
    "        meter = pyln.Meter(sr)\n",
    "        loudness = meter.integrated_loudness(y)\n",
    "        y = pyln.normalize.loudness(y, loudness, DataConfig.LOUDNESS_LUFS)\n",
    "\n",
    "        if np.abs(y).max() < 1e-5:\n",
    "            logging.warning(f\"Silent audio detected: {file_path}\")\n",
    "            return []\n",
    "\n",
    "        segment_samples = int(segment_length * sr)\n",
    "        segments = []\n",
    "\n",
    "        if len(y) < segment_samples:\n",
    "            padded = np.pad(y, (0, segment_samples - len(y)), \"constant\")\n",
    "            segments.append(padded)\n",
    "        else:\n",
    "            step_size = max(1, int(segment_samples * (1 - overlap_ratio)))\n",
    "            for i in range(0, len(y) - segment_samples + 1, step_size):\n",
    "                segments.append(y[i : i + segment_samples])\n",
    "\n",
    "        return segments\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def _audio_to_mel_spectrogram(\n",
    "    y: np.ndarray,\n",
    "    sr: int = DataConfig.SR,\n",
    "    n_fft: int = DataConfig.N_FFT,\n",
    "    hop_length: int = DataConfig.HOP_LENGTH,\n",
    "    n_mels: int = DataConfig.N_MELS,\n",
    "    fmin: float = DataConfig.FMIN,\n",
    "    fmax: float = DataConfig.FMAX,\n",
    "    max_frames_spec: int = 313,\n",
    "    random_crop: bool = False,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Convert waveform to Mel-spectrogram with fixed time axis.\"\"\"\n",
    "    if y is None or len(y) == 0:\n",
    "        return np.full(\n",
    "            (n_mels, max_frames_spec),\n",
    "            DataConfig.MASK_REPLACEMENT_VALUE,\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y=y,\n",
    "        sr=sr,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        n_mels=n_mels,\n",
    "        fmin=fmin,\n",
    "        fmax=fmax,\n",
    "    )\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "    current_frames = mel_spec_db.shape[1]\n",
    "    if current_frames > max_frames_spec:\n",
    "        if random_crop:\n",
    "            start = random.randint(0, current_frames - max_frames_spec)\n",
    "            mel_spec_db = mel_spec_db[:, start : start + max_frames_spec]\n",
    "            logging.debug(\n",
    "                f\"Randomly cropped spectrogram from {current_frames} to {max_frames_spec} frames\"\n",
    "            )\n",
    "        else:\n",
    "            mel_spec_db = mel_spec_db[:, :max_frames_spec]\n",
    "            logging.debug(\n",
    "                f\"Truncated spectrogram from {current_frames} to {max_frames_spec} frames\"\n",
    "            )\n",
    "    elif current_frames < max_frames_spec:\n",
    "        padding = max_frames_spec - current_frames\n",
    "        mel_spec_db = np.pad(\n",
    "            mel_spec_db,\n",
    "            ((0, 0), (0, padding)),\n",
    "            mode=\"constant\",\n",
    "            constant_values=DataConfig.MASK_REPLACEMENT_VALUE,\n",
    "        )\n",
    "        logging.debug(\n",
    "            f\"Padded spectrogram from {current_frames} to {max_frames_spec} frames\"\n",
    "        )\n",
    "        \n",
    "    return mel_spec_db\n",
    "\n",
    "\n",
    "def _compute_global_stats(\n",
    "    filepaths: List[Optional[str]], \n",
    "    segment_length: float, \n",
    "    max_frames_spec: int\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"Compute global mean and std of spectrograms for normalization.\"\"\"\n",
    "    means, stds = [], []\n",
    "    for file_path in filepaths:\n",
    "        segments = _load_and_segment_audio(file_path, segment_length=segment_length)\n",
    "        if not segments:\n",
    "            continue\n",
    "        mel_spec = _audio_to_mel_spectrogram(\n",
    "            segments[0], max_frames_spec=max_frames_spec\n",
    "        )\n",
    "        means.append(mel_spec.mean())\n",
    "        stds.append(mel_spec.std())\n",
    "\n",
    "    return float(np.mean(means)), float(np.mean(stds) + DataConfig.NORM_EPSILON)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7953dc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpecAugment(torch.nn.Module):\n",
    "    \"\"\"Implements SpecAugment for spectrogram augmentation.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.freq_mask = T.FrequencyMasking(freq_mask_param=DataConfig.FREQ_MASK_MAX_WIDTH)\n",
    "        self.time_mask = T.TimeMasking(time_mask_param=DataConfig.TIME_MASK_MAX_WIDTH)\n",
    "        self.num_freq_masks = DataConfig.NUM_FREQ_MASKS\n",
    "        self.num_time_masks = DataConfig.NUM_TIME_MASKS\n",
    "\n",
    "    def forward(self, spec: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Apply frequency and time masking to spectrogram.\"\"\"\n",
    "        if spec.ndim == 4:\n",
    "            spec = spec.squeeze(1)\n",
    "        elif spec.ndim == 3 and spec.shape[0] == 1:\n",
    "            spec = spec.squeeze(0).clone()\n",
    "        elif spec.ndim not in [2, 3]:\n",
    "            logging.warning(\n",
    "                f\"Unexpected spectrogram shape: {spec.shape}. Skipping augmentation.\"\n",
    "            )\n",
    "            return spec\n",
    "\n",
    "        for _ in range(self.num_freq_masks):\n",
    "            spec = (\n",
    "                self.freq_mask(spec)\n",
    "                if spec.ndim == 2\n",
    "                else T.FrequencyMasking(self.freq_mask.freq_mask_param)(spec)\n",
    "            )\n",
    "\n",
    "        for _ in range(self.num_time_masks):\n",
    "            spec = (\n",
    "                self.time_mask(spec)\n",
    "                if spec.ndim == 2\n",
    "                else T.TimeMasking(self.time_mask.time_mask_param)(spec)\n",
    "            )\n",
    "\n",
    "        if spec.ndim == 2:\n",
    "            spec = spec.unsqueeze(0)\n",
    "        return spec\n",
    "\n",
    "class WaveformAugment:\n",
    "    \"\"\"Implements waveform-level augmentations.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.sr = DataConfig.SR\n",
    "        self.pitch_shift = T.PitchShift(sample_rate=self.sr, n_steps=2)\n",
    "\n",
    "    def apply(self, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Apply random waveform augmentations.\"\"\"\n",
    "        y_tensor = torch.from_numpy(y).float()\n",
    "\n",
    "        if random.random() < 0.3:\n",
    "            noise = torch.randn_like(y_tensor) * 0.005\n",
    "            y_tensor = y_tensor + noise\n",
    "\n",
    "        if random.random() < 0.3:\n",
    "            y_tensor = self.pitch_shift(y_tensor.unsqueeze(0)).squeeze(0)\n",
    "\n",
    "        if random.random() < 0.3:\n",
    "            rate = random.uniform(0.8, 1.2)\n",
    "            y_numpy = librosa.effects.time_stretch(y_tensor.detach().numpy(), rate=rate)\n",
    "            original_len = len(y)\n",
    "            if len(y_numpy) < original_len:\n",
    "                y_numpy = np.pad(y_numpy, (0, original_len - len(y_numpy)), \"constant\")\n",
    "            elif len(y_numpy) > original_len:\n",
    "                y_numpy = y_numpy[:original_len]\n",
    "            y_tensor = torch.from_numpy(y_numpy).float()\n",
    "\n",
    "        return y_tensor.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dac0cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConfig:\n",
    "    \"\"\"Configuration for model-specific dataset creation.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        audio_length_seconds: float,\n",
    "        overlap_ratio: float,\n",
    "        apply_augmentation: bool = False,\n",
    "        apply_waveform_augmentation: bool = False,\n",
    "    ):\n",
    "        self.name = name\n",
    "        self.audio_length_seconds = audio_length_seconds\n",
    "        self.overlap_ratio = overlap_ratio\n",
    "        self.apply_augmentation = apply_augmentation\n",
    "        self.apply_waveform_augmentation = apply_waveform_augmentation\n",
    "        self.max_frames_spec = int(\n",
    "            np.ceil((audio_length_seconds * DataConfig.SR) / DataConfig.HOP_LENGTH)\n",
    "        )\n",
    "        logging.info(\n",
    "            f\"ModelConfig for {name}: audio_length={audio_length_seconds}s, \"\n",
    "            f\"max_frames={self.max_frames_spec}, overlap_ratio={overlap_ratio}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a910f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dataset Creation ---\n",
    "class DatasetCreator:\n",
    "    \"\"\"Manages the creation of cached datasets for models.\"\"\"\n",
    "\n",
    "    def __init__(self, model_configs: List[ModelConfig]):\n",
    "        self.model_configs = model_configs\n",
    "        self.label_mapping = {\"real\": 0, \"fake\": 1}\n",
    "        self.spec_augmenter = SpecAugment()\n",
    "        self.waveform_augmenter = WaveformAugment()\n",
    "\n",
    "    def load_metadata(self, set_type: str) -> pd.DataFrame:\n",
    "        \"\"\"Load metadata for a given set type (train/val/test).\"\"\"\n",
    "        metadata_path = os.path.join(\n",
    "            DataConfig.DATASET_ROOT, \n",
    "            set_type, \n",
    "            f\"combined_metadata_{set_type}.csv\"\n",
    "        )\n",
    "        if not os.path.exists(metadata_path):\n",
    "            logging.error(f\"Metadata file not found: {metadata_path}\")\n",
    "            return pd.DataFrame()\n",
    "        # Đảm bảo cột 'path' không có giá trị NaN và reset index\n",
    "        df = pd.read_csv(metadata_path).dropna(subset=[\"path\"]).reset_index(drop=True)\n",
    "        logging.info(f\"Loaded {len(df)} samples from {metadata_path}\")\n",
    "        return df\n",
    "\n",
    "    def validate_and_get_full_path(\n",
    "        self, \n",
    "        set_type: str,\n",
    "        audio_path_relative: str\n",
    "    ) -> Optional[str]:\n",
    "        \"\"\"Validate audio file path and return full path.\n",
    "        audio_path_relative đã bao gồm cả thư mục con (ví dụ: train/fake/tts_basic/...).\n",
    "        \"\"\"\n",
    "        # Đường dẫn đầy đủ sẽ là DATASET_ROOT  / audio_path_relative\n",
    "        # Ví dụ: dataset / train/fake/tts_basic/... \n",
    "        audio_path_obj = Path(audio_path_relative)\n",
    "        if audio_path_obj.parts[1] == \"real\":\n",
    "            full_path = os.path.join(DataConfig.DATASET_ROOT, audio_path_relative)\n",
    "        else:\n",
    "            full_path = os.path.join(DataConfig.DATASET_ROOT, set_type, audio_path_relative)\n",
    "            \n",
    "        # Chuyển đổi dấu gạch chéo để tương thích với hệ điều hành hiện tại\n",
    "        full_path = Path(full_path).as_posix()  # Chuyển đổi sang format Unix-style\n",
    "        full_path = os.path.normpath(full_path)  # Chuẩn hóa đường dẫn\n",
    "\n",
    "        if not os.path.exists(full_path):\n",
    "            logging.warning(f\"Audio file not found: {full_path}\")\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            info = sf.info(full_path)\n",
    "            if info.frames == 0:\n",
    "                logging.warning(f\"Empty audio file: {full_path}\")\n",
    "                return None\n",
    "            y_check, _ = librosa.load(\n",
    "                full_path, sr=DataConfig.SR, mono=True, duration=1.0\n",
    "            )\n",
    "            if np.abs(y_check).max() < 1e-5:\n",
    "                logging.warning(f\"Silent audio detected: {full_path}\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error validating {full_path}: {e}\")\n",
    "            return None\n",
    "        return full_path\n",
    "\n",
    "    def create_cached_datasets(self):\n",
    "        \"\"\"Create cached datasets for each model configuration.\"\"\"\n",
    "        for model_config in self.model_configs:\n",
    "            cache_root = os.path.join(\n",
    "                DataConfig.CACHE_DIR, f\"{model_config.name}_dataset\"\n",
    "            )\n",
    "            os.makedirs(cache_root, exist_ok=True)\n",
    "\n",
    "            for set_type in [\"train\", \"val\", \"test\"]:\n",
    "                logging.info(f\"Processing {set_type} set for {model_config.name}\")\n",
    "                metadata_df = self.load_metadata(set_type)\n",
    "                if metadata_df.empty:\n",
    "                    continue\n",
    "\n",
    "                global_mean, global_std = 0.0, 1.0\n",
    "                if DataConfig.USE_GLOBAL_NORMALIZATION:\n",
    "                    filepaths = [\n",
    "                        self.validate_and_get_full_path(set_type, row[\"path\"])\n",
    "                        for _, row in metadata_df.iterrows()\n",
    "                        if self.validate_and_get_full_path(set_type, row[\"path\"]) is not None\n",
    "                    ]\n",
    "                    if filepaths:\n",
    "                        global_mean, global_std = _compute_global_stats(\n",
    "                            filepaths,\n",
    "                            model_config.audio_length_seconds,\n",
    "                            model_config.max_frames_spec,\n",
    "                        )\n",
    "                        logging.info(\n",
    "                            f\"Global stats ({set_type}): Mean={global_mean:.4f}, Std={global_std:.4f}\"\n",
    "                        )\n",
    "                    else:\n",
    "                        logging.warning(f\"No valid audio files to compute global stats for {set_type}.\")\n",
    "\n",
    "\n",
    "                set_cache_dir = os.path.join(cache_root, set_type)\n",
    "                os.makedirs(set_cache_dir, exist_ok=True)\n",
    "                metadata_records = []\n",
    "\n",
    "                for _, row in metadata_df.iterrows():\n",
    "                    audio_path_relative_in_csv = row[\"path\"]\n",
    "                    label_str = row[\"label\"]\n",
    "                    \n",
    "                    try:\n",
    "                        fake_level = int(row.get(\"fake_level\", 0))\n",
    "                    except (ValueError, TypeError):\n",
    "                        logging.warning(\n",
    "                            f\"Invalid fake_level '{row.get('fake_level', 'N/A')}' for sample {audio_path_relative_in_csv}. Using default 0.\"\n",
    "                        )\n",
    "                        fake_level = 0\n",
    "\n",
    "\n",
    "                    full_path = self.validate_and_get_full_path(set_type, audio_path_relative_in_csv)\n",
    "                    if not full_path:\n",
    "                        continue\n",
    "\n",
    "                    segments = _load_and_segment_audio(\n",
    "                        full_path,\n",
    "                        segment_length=model_config.audio_length_seconds,\n",
    "                        overlap_ratio=model_config.overlap_ratio,\n",
    "                    )\n",
    "                    if not segments:\n",
    "                        continue\n",
    "\n",
    "                    for seg_idx, seg in enumerate(segments):\n",
    "                        processed_seg = seg\n",
    "                        if (\n",
    "                            model_config.apply_waveform_augmentation\n",
    "                            and set_type == \"train\"\n",
    "                        ):\n",
    "                            processed_seg = self.waveform_augmenter.apply(processed_seg)\n",
    "\n",
    "                        mel_spec = _audio_to_mel_spectrogram(\n",
    "                            processed_seg,\n",
    "                            max_frames_spec=model_config.max_frames_spec,\n",
    "                            random_crop=DataConfig.USE_RANDOM_CROPPING and set_type == \"train\",\n",
    "                        )\n",
    "                        mel_spec_tensor = torch.from_numpy(mel_spec).float()\n",
    "\n",
    "                        if DataConfig.USE_GLOBAL_NORMALIZATION:\n",
    "                            mel_spec_tensor = (\n",
    "                                mel_spec_tensor - global_mean\n",
    "                            ) / global_std\n",
    "                        else:\n",
    "                            mean_val = mel_spec_tensor.mean()\n",
    "                            std_val = mel_spec_tensor.std() + DataConfig.NORM_EPSILON\n",
    "                            mel_spec_tensor = (mel_spec_tensor - mean_val) / std_val\n",
    "\n",
    "                        if model_config.apply_augmentation and set_type == \"train\":\n",
    "                            mel_spec_tensor = self.spec_augmenter(\n",
    "                                mel_spec_tensor.unsqueeze(0)\n",
    "                            ).squeeze(0)\n",
    "\n",
    "                        label_dir = \"real\" if label_str == \"real\" else \"fake\"\n",
    "                        sample_cache_dir = os.path.join(set_cache_dir, label_dir)\n",
    "                        os.makedirs(sample_cache_dir, exist_ok=True)\n",
    "\n",
    "                        file_hash = hashlib.md5(\n",
    "                            f\"{full_path}_{seg_idx}\".encode()\n",
    "                        ).hexdigest()\n",
    "                        npy_path = os.path.join(sample_cache_dir, f\"{file_hash}.npy\")\n",
    "                        np.save(npy_path, mel_spec_tensor.numpy())\n",
    "\n",
    "                        metadata_records.append(\n",
    "                            {\n",
    "                                \"npy_path\": npy_path,\n",
    "                                \"original_path\": audio_path_relative_in_csv,\n",
    "                                \"label\": self.label_mapping[label_str],\n",
    "                                \"fake_level\": fake_level,\n",
    "                                \"segment_index\": seg_idx,\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "                if metadata_records:\n",
    "                    metadata_df_processed = pd.DataFrame(metadata_records)\n",
    "                    metadata_output_path = os.path.join(set_cache_dir, \"metadata.csv\")\n",
    "                    metadata_df_processed.to_csv(metadata_output_path, index=False)\n",
    "                    logging.info(\n",
    "                        f\"Saved {len(metadata_df_processed)} samples to {metadata_output_path}\"\n",
    "                    )\n",
    "                else:\n",
    "                    logging.warning(f\"No processed samples for {set_type} for {model_config.name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c79c475",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 07:33:49,431 - INFO - Starting dataset caching process...\n",
      "2025-06-04 07:33:49,432 - INFO - ModelConfig for cnn_performance: audio_length=4.096s, max_frames=128, overlap_ratio=0.75\n",
      "2025-06-04 07:33:49,441 - INFO - Processing test set for cnn_performance\n",
      "2025-06-04 07:33:49,463 - INFO - Loaded 9114 samples from F:\\Deepfake-Audio-Detector\\dataset\\test\\combined_metadata_test.csv\n",
      "f:\\Deepfake-Audio-Detector\\audio-env\\lib\\site-packages\\pyloudnorm\\normalize.py:62: UserWarning: Possible clipped samples in output.\n",
      "  warnings.warn(\"Possible clipped samples in output.\")\n",
      "f:\\Deepfake-Audio-Detector\\audio-env\\lib\\site-packages\\pyloudnorm\\normalize.py:62: UserWarning: Possible clipped samples in output.\n",
      "  warnings.warn(\"Possible clipped samples in output.\")\n",
      "2025-06-04 07:40:12,896 - INFO - Saved 57893 samples to F:\\Deepfake-Audio-Detector\\processed_dataset\\cnn_performance_dataset\\test\\metadata.csv\n",
      "2025-06-04 07:40:12,905 - INFO - Dataset caching process completed.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    logging.info(\"Starting dataset caching process...\")\n",
    "\n",
    "    if os.path.exists(DataConfig.CACHE_DIR):\n",
    "        import shutil\n",
    "        shutil.rmtree(DataConfig.CACHE_DIR)\n",
    "        logging.info(f\"Removed existing {DataConfig.CACHE_DIR} directory.\")\n",
    "    os.makedirs(DataConfig.CACHE_DIR, exist_ok=True)\n",
    "\n",
    "    vit_config_balanced = ModelConfig(\n",
    "        name=\"vit_balanced\",\n",
    "        audio_length_seconds=8.192,\n",
    "        overlap_ratio=0.5,\n",
    "        apply_augmentation=True,\n",
    "        apply_waveform_augmentation=True,\n",
    "    )\n",
    "    \n",
    "    cnn_config_balanced = ModelConfig(\n",
    "        name=\"cnn_balanced\",\n",
    "        audio_length_seconds=8.192, \n",
    "        overlap_ratio=0.5, \n",
    "        apply_augmentation=True,\n",
    "        apply_waveform_augmentation=True,\n",
    "    )\n",
    "\n",
    "    vit_config_performance = ModelConfig(\n",
    "        name=\"vit_performance\",\n",
    "        audio_length_seconds=10.24,\n",
    "        overlap_ratio=0.0,\n",
    "        apply_augmentation=True,\n",
    "        apply_waveform_augmentation=True,\n",
    "    )\n",
    "\n",
    "    cnn_config_performance = ModelConfig(\n",
    "        name=\"cnn_performance\",\n",
    "        audio_length_seconds=4.096,\n",
    "        overlap_ratio=0.75,\n",
    "        apply_augmentation=True,\n",
    "        apply_waveform_augmentation=True,\n",
    "    )\n",
    "\n",
    "    model_configurations = [\n",
    "        vit_config_balanced,\n",
    "        cnn_config_balanced,\n",
    "        vit_config_performance,\n",
    "        cnn_config_performance,\n",
    "    ]\n",
    "\n",
    "    creator = DatasetCreator(model_configurations)\n",
    "    creator.create_cached_datasets()\n",
    "\n",
    "    logging.info(\"Dataset caching process completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e06f590",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
