{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21080510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "import wandb\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c95102c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: WARNING If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "wandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Nekloyh\\_netrc\n",
      "wandb: Currently logged in as: nekloyh (nekloyh-none) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WandB login successful using wandb_api_key.\n"
     ]
    }
   ],
   "source": [
    "# --- WandB login ---\n",
    "try:\n",
    "    with open(\"wandb_api_key.txt\", \"r\") as file:\n",
    "        wandb_api_key = file.read().strip()\n",
    "    wandb.login(key=wandb_api_key)\n",
    "    print(\"WandB login successful using wandb_api_key.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to login to WandB: {e}. Falling back to manual login.\")\n",
    "    wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c35ccef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration Section ---\n",
    "class DataConfig:\n",
    "    \"\"\"Configuration for audio data processing parameters.\"\"\"\n",
    "    # General audio processing settings\n",
    "    SEED = 42  # Random seed for reproducibility\n",
    "\n",
    "    SR = 16000  # Sample rate (Hz)\n",
    "    N_FFT = 2048  # FFT window size\n",
    "    HOP_LENGTH = 512  # Hop length for spectrogram\n",
    "    N_MELS = 128  # Number of Mel bands\n",
    "    FMIN = 0.0  # Minimum frequency (Hz)\n",
    "    FMAX = 8000.0  # Maximum frequency (Hz)\n",
    "\n",
    "    # Augmentation settings\n",
    "    NUM_TIME_MASKS = 2  # Number of time masks for SpecAugment\n",
    "    NUM_FREQ_MASKS = 2  # Number of frequency masks for SpecAugment\n",
    "    TIME_MASK_MAX_WIDTH = 60  # Maximum width of time mask\n",
    "    FREQ_MASK_MAX_WIDTH = 25  # Maximum width of frequency mask\n",
    "    MASK_REPLACEMENT_VALUE = -80.0  # Value for masked regions in spectrogram\n",
    "    NORM_EPSILON = 1e-6  # Small value to prevent division by zero\n",
    "    LOUDNESS_LUFS = -23.0  # Target loudness (LUFS)\n",
    "\n",
    "    # Dataset and processing options\n",
    "    USE_GLOBAL_NORMALIZATION = False  # Use global mean/std for normalization\n",
    "    USE_RANDOM_CROPPING = True  # Apply random cropping to spectrograms\n",
    "    CACHE_DIR = \"F:\\\\Deepfake-Audio-Detector\\\\processed_dataset\" # Directory for processed data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ab1d80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConfig:\n",
    "    \"\"\"Configuration for model-specific dataset creation.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        audio_length_seconds: float,\n",
    "        overlap_ratio: float,\n",
    "        apply_augmentation: bool = False,\n",
    "        apply_waveform_augmentation: bool = False,\n",
    "    ):\n",
    "        self.name = name\n",
    "        self.audio_length_seconds = audio_length_seconds\n",
    "        self.overlap_ratio = overlap_ratio\n",
    "        self.apply_augmentation = apply_augmentation\n",
    "        self.apply_waveform_augmentation = apply_waveform_augmentation\n",
    "        # Calculate max_frames_spec\n",
    "        frames = (audio_length_seconds * DataConfig.SR) / DataConfig.HOP_LENGTH\n",
    "        self.max_frames_spec = int(np.ceil(frames))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7e6393",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_MODEL_CONFIGS = {\n",
    "    \"cnn_balanced_dataset\": ModelConfig(\n",
    "        name=\"cnn_balanced_dataset\",\n",
    "        audio_length_seconds=8.192,\n",
    "        overlap_ratio=0.5,\n",
    "        apply_augmentation=True,\n",
    "        apply_waveform_augmentation=True,\n",
    "    ),\n",
    "    \"cnn_performance_dataset\": ModelConfig(\n",
    "        name=\"cnn_performance_dataset\",\n",
    "        audio_length_seconds=4.096,\n",
    "        overlap_ratio=0.75,\n",
    "        apply_augmentation=True,\n",
    "        apply_waveform_augmentation=True,\n",
    "    ),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8bddbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. CNN Configuration ---\n",
    "class CNNConfig:\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        image_size: tuple,\n",
    "        channels: int = 1,\n",
    "        num_classes: int = 2,\n",
    "        blocks: list = [3, 4, 6, 3],\n",
    "        filters: list = [64, 128, 256, 512],\n",
    "        bottleneck: bool = False,\n",
    "    ):\n",
    "        self.name = name\n",
    "        self.image_height, self.image_width = image_size\n",
    "        self.channels = channels\n",
    "        self.num_classes = num_classes\n",
    "        self.blocks = blocks\n",
    "        self.filters = filters\n",
    "        self.bottleneck = bottleneck\n",
    "\n",
    "        assert len(blocks) == len(filters), \"Number of blocks must match number of filter stages.\"\n",
    "        assert all(f > 0 for f in filters), \"All filter sizes must be positive.\"\n",
    "        assert all(b >= 0 for b in blocks), \"All block counts must be non-negative.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c8c9e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. CNN Architecture (ResNet-like) ---\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class BottleneckBlock(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class AudioCNN(nn.Module):\n",
    "    def __init__(self, config: CNNConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        block = BottleneckBlock if config.bottleneck else BasicBlock\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(config.channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        layers = []\n",
    "        for num_blocks, out_channels in zip(config.blocks, config.filters):\n",
    "            layers.append(self._make_layer(block, out_channels, num_blocks))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(config.filters[-1] * block.expansion, config.num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, out_channels * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * block.expansion),\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layers(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7110f60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Dataset Class for Cached Data ---\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, cache_dir: str, set_type: str, n_mels: int, max_frames_spec: int):\n",
    "        self.cache_path = os.path.join(cache_dir, set_type)\n",
    "        self.metadata_path = os.path.join(self.cache_path, \"metadata.csv\")\n",
    "        self.n_mels = n_mels\n",
    "        self.max_frames_spec = max_frames_spec\n",
    "\n",
    "        if not os.path.exists(self.metadata_path):\n",
    "            raise FileNotFoundError(\n",
    "                f\"Metadata file not found: {self.metadata_path}. Please run data_preprocessing.py first.\"\n",
    "            )\n",
    "\n",
    "        self.metadata = pd.read_csv(self.metadata_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.metadata.iloc[idx]\n",
    "        npy_path = os.path.join(self.cache_path, row[\"npy_path\"])\n",
    "        label = row[\"label\"]\n",
    "\n",
    "        try:\n",
    "            spectrogram = np.load(npy_path)\n",
    "            # Ensure spectrogram is 3D (channels, height, width) for CNN\n",
    "            if spectrogram.ndim == 2:\n",
    "                spectrogram = np.expand_dims(spectrogram, axis=0)\n",
    "            elif spectrogram.ndim == 3 and spectrogram.shape[0] != 1:\n",
    "                raise ValueError(\n",
    "                    f\"Unexpected spectrogram shape: {spectrogram.shape}. Expected (1, N_MELS, N_FRAMES).\"\n",
    "                )\n",
    "\n",
    "            # Convert to float32 and then to tensor\n",
    "            spectrogram = torch.from_numpy(spectrogram).float()\n",
    "\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"Error loading or processing {npy_path}: {e}\")\n",
    "            return None  # Return None to be filtered by collate_fn\n",
    "\n",
    "        return spectrogram, torch.tensor(label).long()\n",
    "\n",
    "# Custom collate function to filter None values\n",
    "def custom_collate_fn(batch):\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if not batch:\n",
    "        return None\n",
    "    return torch.utils.data.dataloader.default_collate(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52e5da2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Training and Evaluation Functions ---\n",
    "def train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    device,\n",
    "    num_epochs,\n",
    "    run_name,\n",
    "    dataset_name,\n",
    "):\n",
    "    model.to(device)\n",
    "    best_val_f1 = -1\n",
    "\n",
    "    # Initialize wandb run\n",
    "    wandb.init(\n",
    "        project=\"audio-deepfake-detection\",\n",
    "        name=run_name,\n",
    "        config={\n",
    "            \"learning_rate\": optimizer.defaults[\"lr\"],\n",
    "            \"epochs\": num_epochs,\n",
    "            \"batch_size\": train_loader.batch_size,\n",
    "            \"model_name\": model.__class__.__name__,\n",
    "            \"model_config\": model.config.__dict__,\n",
    "            \"dataset_name\": dataset_name,\n",
    "            \"device\": str(device),\n",
    "        },\n",
    "    )\n",
    "    wandb.watch(model, log_freq=100)  # Log gradients and model parameters\n",
    "\n",
    "    print(f\"Starting training for {num_epochs} epochs on {device}...\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        pbar = tqdm(\n",
    "            train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} [Train]\", leave=False\n",
    "        )\n",
    "        for batch_idx, batch in enumerate(pbar):\n",
    "            if batch is None:  # Skip empty batches from collate_fn\n",
    "                continue\n",
    "            data, labels = batch\n",
    "            if -1 in labels.cpu().numpy():  # Check if any -1 label exists\n",
    "                continue\n",
    "\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        val_loss, val_preds, val_labels, val_probs = evaluate_model(\n",
    "            model, val_loader, criterion, device\n",
    "        )\n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        val_f1 = f1_score(val_labels, val_preds, average=\"binary\")\n",
    "        val_roc_auc = roc_auc_score(val_labels, val_probs[:, 1])\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1} Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}, Val ROC AUC: {val_roc_auc:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Log metrics to wandb\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": avg_train_loss,\n",
    "                \"val_loss\": val_loss,\n",
    "                \"val_accuracy\": val_acc,\n",
    "                \"val_f1_score\": val_f1,\n",
    "                \"val_roc_auc\": val_roc_auc,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Save best model\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            model_save_path = f\"best_cnn_model_{run_name}.pth\"\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(f\"Saved best model with F1: {best_val_f1:.4f} to {model_save_path}\")\n",
    "            wandb.save(model_save_path)  # Save model to wandb\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device, return_cm=False):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(data_loader, desc=\"Evaluation\", leave=False)\n",
    "        for batch in pbar:\n",
    "            if batch is None:  # Skip empty batches from collate_fn\n",
    "                continue\n",
    "            data, labels = batch\n",
    "            if -1 in labels.cpu().numpy():  # Check if any -1 label exists\n",
    "                continue\n",
    "\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "\n",
    "    if return_cm:\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        return (\n",
    "            avg_loss,\n",
    "            np.array(all_preds),\n",
    "            np.array(all_labels),\n",
    "            np.array(all_probs),\n",
    "            cm,\n",
    "        )\n",
    "    else:\n",
    "        return avg_loss, np.array(all_preds), np.array(all_labels), np.array(all_probs)\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, labels=[\"Real\", \"Fake\"], run_name=\"\", save_dir=\".\"):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels\n",
    "    )\n",
    "    plt.title(f\"Confusion Matrix for {run_name}\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    save_path = os.path.join(save_dir, f\"confusion_matrix_{run_name}.png\")\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"Confusion matrix saved to {save_path}\")\n",
    "    plt.show()\n",
    "    return save_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fba34103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training Configuration ---\n",
    "class TrainingConfig:\n",
    "    \"\"\"Configuration for training parameters.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_size: str,\n",
    "        dataset_name: str,\n",
    "        epochs: int,\n",
    "        learning_rate: float,\n",
    "        batch_size: int,\n",
    "        num_workers: int,\n",
    "    ):\n",
    "        self.model_size = model_size\n",
    "        self.dataset_name = dataset_name\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        # Validate parameters\n",
    "        assert model_size in ['CNN_Small', 'CNN_Medium', 'CNN_Large'], f\"Model size '{model_size}' not found\"\n",
    "        assert dataset_name in ALL_MODEL_CONFIGS, f\"Dataset name '{dataset_name}' not found in ALL_MODEL_CONFIGS\"\n",
    "        assert batch_size > 0, \"Batch size must be positive\"\n",
    "        assert epochs > 0, \"Number of epochs must be positive\"\n",
    "        assert learning_rate > 0, \"Learning rate must be positive\"\n",
    "        assert num_workers >= 0, \"Number of workers must be non-negative\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f52ff6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(training_config):\n",
    "    \"\"\"Main function to run the training process.\"\"\"\n",
    "\n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(DataConfig.SEED)\n",
    "    np.random.seed(DataConfig.SEED)\n",
    "\n",
    "    # Get configuration from TrainingConfig\n",
    "    model_size = training_config.model_size\n",
    "    dataset_name = training_config.dataset_name\n",
    "    epochs = training_config.epochs\n",
    "    learning_rate = training_config.learning_rate\n",
    "    batch_size = training_config.batch_size\n",
    "    num_workers = training_config.num_workers\n",
    "\n",
    "    # Get the ModelConfig for the chosen dataset\n",
    "    if dataset_name not in ALL_MODEL_CONFIGS:\n",
    "        print(f\"Error: Dataset name '{dataset_name}' not found in ALL_MODEL_CONFIGS.\")\n",
    "        print(\"Please ensure 'data_preprocessing.py' defines this dataset name.\")\n",
    "        return\n",
    "\n",
    "    current_dataset_model_config = ALL_MODEL_CONFIGS[dataset_name]\n",
    "    max_frames_spec = current_dataset_model_config.max_frames_spec\n",
    "\n",
    "    # Device configuration\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Determine CNN configuration based on model_size\n",
    "    cnn_configs = {\n",
    "        \"CNN_Small\": CNNConfig(\n",
    "            name=\"CNN_Small\",\n",
    "            image_size=(DataConfig.N_MELS, current_dataset_model_config.max_frames_spec),\n",
    "            blocks=[3, 4, 6, 3],\n",
    "            filters=[64, 128, 256, 512],\n",
    "            bottleneck=False,\n",
    "        ),\n",
    "        \"CNN_Medium\": CNNConfig(\n",
    "            name=\"CNN_Medium\",\n",
    "            image_size=(DataConfig.N_MELS, current_dataset_model_config.max_frames_spec),\n",
    "            blocks=[3, 4, 6, 3],\n",
    "            filters=[64, 128, 256, 512],\n",
    "            bottleneck=True,\n",
    "        ),\n",
    "        \"CNN_Large\": CNNConfig(\n",
    "            name=\"CNN_Large\",\n",
    "            image_size=(DataConfig.N_MELS, current_dataset_model_config.max_frames_spec),\n",
    "            blocks=[3, 6, 20, 3],\n",
    "            filters=[64, 128, 256, 512],\n",
    "            bottleneck=True,\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    if model_size not in cnn_configs:\n",
    "        print(\n",
    "            f\"Error: Invalid model_size '{model_size}'. Choose from {list(cnn_configs.keys())}\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    cnn_config = cnn_configs[model_size]\n",
    "    print(f\"Configuring {cnn_config.name} model...\")\n",
    "    print(\n",
    "        f\"Image size: {cnn_config.image_height}x{cnn_config.image_width}, Blocks: {cnn_config.blocks}, Filters: {cnn_config.filters}\"\n",
    "    )\n",
    "\n",
    "    # Dataset paths\n",
    "    base_cache_dir = DataConfig.CACHE_DIR\n",
    "    model_cache_dir = os.path.join(base_cache_dir, dataset_name)\n",
    "\n",
    "    print(f\"Loading data from: {model_cache_dir}\")\n",
    "\n",
    "    # Datasets\n",
    "    train_dataset = AudioDataset(\n",
    "        model_cache_dir, \"train\", DataConfig.N_MELS, max_frames_spec\n",
    "    )\n",
    "    val_dataset = AudioDataset(\n",
    "        model_cache_dir, \"val\", DataConfig.N_MELS, max_frames_spec\n",
    "    )\n",
    "    test_dataset = AudioDataset(\n",
    "        model_cache_dir, \"test\", DataConfig.N_MELS, max_frames_spec\n",
    "    )\n",
    "\n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        collate_fn=custom_collate_fn,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        collate_fn=custom_collate_fn,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        collate_fn=custom_collate_fn,\n",
    "    )\n",
    "\n",
    "    print(f\"Train samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    print(f\"Test samples: {len(test_dataset)}\")\n",
    "    print(f\"Using Batch size: {batch_size}\")\n",
    "\n",
    "    # Initialize model, loss, and optimizer\n",
    "    model = AudioCNN(config=cnn_config)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Run name for W&B\n",
    "    run_name = f\"{model_size}_{dataset_name}_{datetime.now().strftime('%H%M%S')}\"\n",
    "\n",
    "    # Train the model\n",
    "    trained_model = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        device,\n",
    "        epochs,\n",
    "        run_name,\n",
    "        dataset_name,\n",
    "    )\n",
    "\n",
    "    # Evaluate on test set\n",
    "    print(f\"\\n--- Evaluating {cnn_config.name} on Test Set ({dataset_name}) ---\")\n",
    "    test_loss, test_preds, test_labels, test_probs, test_cm = evaluate_model(\n",
    "        trained_model, test_loader, criterion, device, return_cm=True\n",
    "    )\n",
    "\n",
    "    test_acc = accuracy_score(test_labels, test_preds)\n",
    "    test_f1 = f1_score(test_labels, test_preds, average=\"binary\")\n",
    "    test_roc_auc = roc_auc_score(test_labels, test_probs[:, 1])\n",
    "\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"Test F1-score: {test_f1:.4f}\")\n",
    "    print(f\"Test ROC AUC: {test_roc_auc:.4f}\")\n",
    "\n",
    "    # Plot and save confusion matrix\n",
    "    cm_plot_path = plot_confusion_matrix(test_cm, run_name=run_name, save_dir=\"results\")\n",
    "\n",
    "    # Log test metrics to W&B\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"test_loss\": test_loss,\n",
    "            \"test_accuracy\": test_acc,\n",
    "            \"test_f1_score\": test_f1,\n",
    "            \"test_roc_auc\": test_roc_auc,\n",
    "            \"confusion_matrix\": wandb.Image(cm_plot_path),\n",
    "        }\n",
    "    )\n",
    "    wandb.finish()\n",
    "\n",
    "    return trained_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cad270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CNN Training Configuration ===\n",
      "Model Size: CNN_Small\n",
      "Dataset: cnn_balanced_dataset\n",
      "Epochs: 20\n",
      "Learning Rate: 0.0001\n",
      "Batch Size: 32\n",
      "Num Workers: 8\n",
      "==================================\n",
      "Using device: cuda\n",
      "Configuring CNN_Small model...\n",
      "Image size: 128x250, Blocks: [3, 4, 6, 3], Filters: [64, 128, 256, 512]\n",
      "Loading data from: F:\\Deepfake-Audio-Detector\\processed_dataset\\cnn_balanced_dataset\n",
      "Train samples: 61022\n",
      "Validation samples: 13205\n",
      "Test samples: 13650\n",
      "Using Batch size: 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>f:\\Deepfake-Audio-Detector\\notebooks\\wandb\\run-20250603_131339-k29neznl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nekloyh-none/audio-deepfake-detection/runs/k29neznl' target=\"_blank\">CNN_Small_cnn_balanced_dataset_131339</a></strong> to <a href='https://wandb.ai/nekloyh-none/audio-deepfake-detection' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nekloyh-none/audio-deepfake-detection' target=\"_blank\">https://wandb.ai/nekloyh-none/audio-deepfake-detection</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nekloyh-none/audio-deepfake-detection/runs/k29neznl' target=\"_blank\">https://wandb.ai/nekloyh-none/audio-deepfake-detection/runs/k29neznl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 20 epochs on cuda...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 [Train]:   0%|          | 0/1907 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Configure training parameters\n",
    "training_params = {\n",
    "    \"model_size\": \"CNN_Small\",\n",
    "    \"dataset_name\": \"cnn_balanced_dataset\",\n",
    "    \"epochs\": 20,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_workers\": 8,\n",
    "}\n",
    "\n",
    "# Initialize TrainingConfig with the specified parameters\n",
    "training_config = TrainingConfig(**training_params)\n",
    "\n",
    "# Print training configuration\n",
    "print(\"=== CNN Training Configuration ===\")\n",
    "print(f\"Model Size: {training_config.model_size}\")\n",
    "print(f\"Dataset: {training_config.dataset_name}\")\n",
    "print(f\"Epochs: {training_config.epochs}\")\n",
    "print(f\"Learning Rate: {training_config.learning_rate}\")\n",
    "print(f\"Batch Size: {training_config.batch_size}\")\n",
    "print(f\"Num Workers: {training_config.num_workers}\")\n",
    "print(\"==================================\")\n",
    "\n",
    "# Run training with the configured TrainingConfig\n",
    "trained_model = run_training(training_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8610f3af",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
