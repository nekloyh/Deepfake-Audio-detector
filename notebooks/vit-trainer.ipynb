{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21080510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "import wandb\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c95102c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: WARNING If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "wandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Nekloyh\\_netrc\n",
      "wandb: Currently logged in as: nekloyh (nekloyh-none) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WandB login successful using wandb_api_key.\n"
     ]
    }
   ],
   "source": [
    "# --- WandB login ---\n",
    "try:\n",
    "    with open(\"wandb_api_key.txt\", \"r\") as file:\n",
    "        wandb_api_key = file.read().strip()\n",
    "    wandb.login(key=wandb_api_key)\n",
    "    print(\"WandB login successful using wandb_api_key.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to login to WandB: {e}. Falling back to manual login.\")\n",
    "    wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35ccef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration Section ---\n",
    "class DataConfig:\n",
    "    \"\"\"Configuration for audio data processing parameters.\"\"\"\n",
    "    # General audio processing settings\n",
    "    SEED = 42  # Random seed for reproducibility\n",
    "\n",
    "    SR = 16000  # Sample rate (Hz)\n",
    "    N_FFT = 2048  # FFT window size\n",
    "    HOP_LENGTH = 512  # Hop length for spectrogram\n",
    "    N_MELS = 128  # Number of Mel bands\n",
    "    FMIN = 0.0  # Minimum frequency (Hz)\n",
    "    FMAX = 8000.0  # Maximum frequency (Hz)\n",
    "\n",
    "    # Augmentation settings\n",
    "    NUM_TIME_MASKS = 2  # Number of time masks for SpecAugment\n",
    "    NUM_FREQ_MASKS = 2  # Number of frequency masks for SpecAugment\n",
    "    TIME_MASK_MAX_WIDTH = 60  # Maximum width of time mask\n",
    "    FREQ_MASK_MAX_WIDTH = 25  # Maximum width of frequency mask\n",
    "    MASK_REPLACEMENT_VALUE = -80.0  # Value for masked regions in spectrogram\n",
    "    NORM_EPSILON = 1e-6  # Small value to prevent division by zero\n",
    "    LOUDNESS_LUFS = -23.0  # Target loudness (LUFS)\n",
    "\n",
    "    # Dataset and processing options\n",
    "    USE_GLOBAL_NORMALIZATION = False  # Use global mean/std for normalization\n",
    "    USE_RANDOM_CROPPING = True  # Apply random cropping to spectrograms\n",
    "    CACHE_DIR = \"F:\\\\Deepfake-Audio-Detector\\\\processed_dataset\" # Directory for processed data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab1d80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConfig:\n",
    "    \"\"\"Configuration for model-specific dataset creation.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        audio_length_seconds: float,\n",
    "        overlap_ratio: float,\n",
    "        apply_augmentation: bool = False,\n",
    "        apply_waveform_augmentation: bool = False,\n",
    "        patch_width: int = 16,  # Must match ViTConfig patch_width\n",
    "    ):\n",
    "        self.name = name\n",
    "        self.audio_length_seconds = audio_length_seconds\n",
    "        self.overlap_ratio = overlap_ratio\n",
    "        self.apply_augmentation = apply_augmentation\n",
    "        self.apply_waveform_augmentation = apply_waveform_augmentation\n",
    "        # Calculate max_frames_spec and ensure divisibility by patch_width\n",
    "        frames = (audio_length_seconds * DataConfig.SR) / DataConfig.HOP_LENGTH\n",
    "        self.max_frames_spec = int(np.ceil(frames / patch_width) * patch_width)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7e6393",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_MODEL_CONFIGS = {\n",
    "    \"vit_balanced_dataset\": ModelConfig(\n",
    "        name=\"vit_balanced_dataset\",\n",
    "        audio_length_seconds=8.192,\n",
    "        overlap_ratio=0.5,\n",
    "        apply_augmentation=True,\n",
    "        apply_waveform_augmentation=True,\n",
    "    ),\n",
    "    \"vit_performance_dataset\": ModelConfig(\n",
    "        name=\"vit_performance_dataset\",\n",
    "        audio_length_seconds=10.24,  \n",
    "        overlap_ratio=0.0,\n",
    "        apply_augmentation=True,\n",
    "        apply_waveform_augmentation=True,\n",
    "    ),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bddbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Cấu hình Model ViT ---\n",
    "class ViTConfig:\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        image_size: tuple,\n",
    "        patch_size: tuple,\n",
    "        dim: int,\n",
    "        depth: int,\n",
    "        heads: int,\n",
    "        mlp_dim: int,\n",
    "        dropout: float = 0.1,\n",
    "        emb_dropout: float = 0.1,\n",
    "        channels: int = 1,\n",
    "        num_classes: int = 2,\n",
    "    ):\n",
    "        self.name = name\n",
    "        self.image_height, self.image_width = image_size\n",
    "        self.patch_height, self.patch_width = patch_size\n",
    "        self.dim = dim\n",
    "        self.depth = depth\n",
    "        self.heads = heads\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.dropout = dropout\n",
    "        self.emb_dropout = emb_dropout\n",
    "        self.channels = channels\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        assert self.image_height % self.patch_height == 0 and self.image_width % self.patch_width == 0, \\\n",
    "            \"Image dimensions must be divisible by the patch size.\"\n",
    "        assert dim > 0, \"Embedding dimension must be positive.\"\n",
    "        assert depth > 0, \"Number of transformer layers must be positive.\"\n",
    "        assert heads > 0, \"Number of attention heads must be positive.\"\n",
    "        assert mlp_dim > 0, \"MLP dimension must be positive.\"\n",
    "\n",
    "        self.num_patches = (self.image_height // self.patch_height) * (self.image_width // self.patch_width)\n",
    "        self.patch_dim = self.channels * self.patch_height * self.patch_width\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8c9e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Kiến trúc Mô hình Vision Transformer (AudioViT) ---\n",
    "# Tái cấu trúc để sử dụng einops\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads=8, dim_head=64, dropout=0.0):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head**-0.5\n",
    "\n",
    "        self.attend = nn.Softmax(dim=-1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
    "\n",
    "        self.to_out = (\n",
    "            nn.Sequential(nn.Linear(inner_dim, dim), nn.Dropout(dropout))\n",
    "            if project_out\n",
    "            else nn.Identity()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: rearrange(t, \"b n (h d) -> b h n d\", h=self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = rearrange(torch.matmul(attn, v), \"b h n d -> b n (h d)\")\n",
    "        return self.to_out(out)\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, mlp_dim, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(\n",
    "                nn.ModuleList(\n",
    "                    [\n",
    "                        PreNorm(\n",
    "                            dim,\n",
    "                            Attention(\n",
    "                                dim, heads=heads, dim_head=dim // heads, dropout=dropout\n",
    "                            ),\n",
    "                        ),\n",
    "                        PreNorm(dim, FeedForward(dim, mlp_dim, dropout=dropout)),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "\n",
    "class AudioViT(nn.Module):\n",
    "    def __init__(self, *, config: ViTConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        image_height, image_width = config.image_height, config.image_width\n",
    "        patch_height, patch_width = config.patch_height, config.patch_width\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange(\n",
    "                \"b c (h p1) (w p2) -> b (h w) (p1 p2 c)\",\n",
    "                p1=patch_height,\n",
    "                p2=patch_width,\n",
    "            ),\n",
    "            nn.LayerNorm(config.patch_dim),\n",
    "            nn.Linear(config.patch_dim, config.dim),\n",
    "            nn.LayerNorm(config.dim),\n",
    "        )\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(\n",
    "            torch.randn(1, config.num_patches + 1, config.dim)\n",
    "        )\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, config.dim))\n",
    "        self.dropout = nn.Dropout(config.emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(\n",
    "            config.dim, config.depth, config.heads, config.mlp_dim, config.dropout\n",
    "        )\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(config.dim), nn.Linear(config.dim, config.num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.to_patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        cls_tokens = repeat(self.cls_token, \"1 1 d -> b 1 d\", b=b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += self.pos_embedding[:, : (n + 1)]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # Lấy đầu ra của CLS token\n",
    "        x = x[:, 0]\n",
    "\n",
    "        return self.mlp_head(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7110f60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Lớp Dataset cho dữ liệu đã được cache ---\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, cache_dir: str, set_type: str, n_mels: int, max_frames_spec: int):\n",
    "        self.cache_path = os.path.join(cache_dir, set_type)\n",
    "        self.metadata_path = os.path.join(self.cache_path, \"metadata.csv\")\n",
    "        self.n_mels = n_mels\n",
    "        self.max_frames_spec = max_frames_spec\n",
    "\n",
    "        if not os.path.exists(self.metadata_path):\n",
    "            raise FileNotFoundError(\n",
    "                f\"Metadata file not found: {self.metadata_path}. Please run data_preprocessing.py first.\"\n",
    "            )\n",
    "\n",
    "        self.metadata = pd.read_csv(self.metadata_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.metadata.iloc[idx]\n",
    "        npy_path = os.path.join(self.cache_path, row[\"npy_path\"])\n",
    "        label = row[\"label\"]\n",
    "\n",
    "        try:\n",
    "            spectrogram = np.load(npy_path)\n",
    "            # Ensure spectrogram is 3D (channels, height, width) for ViT\n",
    "            if spectrogram.ndim == 2:\n",
    "                spectrogram = np.expand_dims(spectrogram, axis=0)\n",
    "            elif spectrogram.ndim == 3 and spectrogram.shape[0] != 1:\n",
    "                raise ValueError(\n",
    "                    f\"Unexpected spectrogram shape: {spectrogram.shape}. Expected (1, N_MELS, N_FRAMES).\"\n",
    "                )\n",
    "\n",
    "            # Convert to float32 and then to tensor\n",
    "            spectrogram = torch.from_numpy(spectrogram).float()\n",
    "\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"Error loading or processing {npy_path}: {e}\")\n",
    "            return None  # Return None to be filtered by collate_fn\n",
    "\n",
    "        return spectrogram, torch.tensor(label).long()\n",
    "\n",
    "# Custom collate function to filter None values\n",
    "def custom_collate_fn(batch):\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if not batch:\n",
    "        return None\n",
    "    return torch.utils.data.dataloader.default_collate(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e5da2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Hàm Huấn luyện và Đánh giá ---\n",
    "def train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    device,\n",
    "    num_epochs,\n",
    "    run_name,\n",
    "    dataset_name,\n",
    "):\n",
    "    model.to(device)\n",
    "    best_val_f1 = -1\n",
    "\n",
    "    # Initialize wandb run\n",
    "    wandb.init(\n",
    "        project=\"audio-deepfake-detection\",\n",
    "        name=run_name,\n",
    "        config={\n",
    "            \"learning_rate\": optimizer.defaults[\"lr\"],\n",
    "            \"epochs\": num_epochs,\n",
    "            \"batch_size\": train_loader.batch_size,\n",
    "            \"model_name\": model.__class__.__name__,\n",
    "            \"model_config\": model.config.__dict__,\n",
    "            \"dataset_name\": dataset_name,\n",
    "            \"device\": str(device),\n",
    "        },\n",
    "    )\n",
    "    wandb.watch(model, log_freq=100)  # Log gradients and model parameters\n",
    "\n",
    "    print(f\"Starting training for {num_epochs} epochs on {device}...\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        pbar = tqdm(\n",
    "            train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} [Train]\", leave=False\n",
    "        )\n",
    "        for batch_idx, batch in enumerate(pbar):\n",
    "            if batch is None:  # Skip empty batches from collate_fn\n",
    "                continue\n",
    "            data, labels = batch\n",
    "            if -1 in labels.cpu().numpy():  # Check if any -1 label exists\n",
    "                continue\n",
    "\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        val_loss, val_preds, val_labels, val_probs = evaluate_model(\n",
    "            model, val_loader, criterion, device\n",
    "        )\n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        val_f1 = f1_score(val_labels, val_preds, average=\"binary\")\n",
    "        val_roc_auc = roc_auc_score(val_labels, val_probs[:, 1])\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1} Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}, Val ROC AUC: {val_roc_auc:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Log metrics to wandb\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": avg_train_loss,\n",
    "                \"val_loss\": val_loss,\n",
    "                \"val_accuracy\": val_acc,\n",
    "                \"val_f1_score\": val_f1,\n",
    "                \"val_roc_auc\": val_roc_auc,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Save best model\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            model_save_path = f\"best_vit_model_{run_name}.pth\"\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(f\"Saved best model with F1: {best_val_f1:.4f} to {model_save_path}\")\n",
    "            wandb.save(model_save_path)  # Save model to wandb\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device, return_cm=False):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(data_loader, desc=\"Evaluation\", leave=False)\n",
    "        for batch in pbar:\n",
    "            if batch is None:  # Skip empty batches from collate_fn\n",
    "                continue\n",
    "            data, labels = batch\n",
    "            if -1 in labels.cpu().numpy():  # Check if any -1 label exists\n",
    "                continue\n",
    "\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "\n",
    "    if return_cm:\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        return (\n",
    "            avg_loss,\n",
    "            np.array(all_preds),\n",
    "            np.array(all_labels),\n",
    "            np.array(all_probs),\n",
    "            cm,\n",
    "        )\n",
    "    else:\n",
    "        return avg_loss, np.array(all_preds), np.array(all_labels), np.array(all_probs)\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, labels=[\"Real\", \"Fake\"], run_name=\"\", save_dir=\".\"):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels\n",
    "    )\n",
    "    plt.title(f\"Confusion Matrix for {run_name}\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    save_path = os.path.join(save_dir, f\"confusion_matrix_{run_name}.png\")\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"Confusion matrix saved to {save_path}\")\n",
    "    plt.show()\n",
    "    return save_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba34103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training Configuration ---\n",
    "class TrainingConfig:\n",
    "    \"\"\"Configuration for training parameters.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_size: str,\n",
    "        dataset_name: str,\n",
    "        epochs: int,\n",
    "        learning_rate: float,\n",
    "        batch_size: int,\n",
    "        num_workers: int,\n",
    "    ):\n",
    "        self.model_size = model_size\n",
    "        self.dataset_name = dataset_name\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        # Validate parameters\n",
    "        assert model_size in ['ViT_Small', 'ViT_Medium', 'ViT_Large'], f\"Model size '{model_size}' not found\"\n",
    "        assert dataset_name in ALL_MODEL_CONFIGS, f\"Dataset name '{dataset_name}' not found in ALL_MODEL_CONFIGS\"\n",
    "        assert batch_size > 0, \"Batch size must be positive\"\n",
    "        assert epochs > 0, \"Number of epochs must be positive\"\n",
    "        assert learning_rate > 0, \"Learning rate must be positive\"\n",
    "        assert num_workers >= 0, \"Number of workers must be non-negative\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52ff6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(training_config):\n",
    "    \"\"\"Main function to run the training process.\"\"\"\n",
    "\n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(DataConfig.SEED)\n",
    "    np.random.seed(DataConfig.SEED)\n",
    "\n",
    "    # Get configuration from TrainingConfig\n",
    "    model_size = training_config.model_size\n",
    "    dataset_name = training_config.dataset_name\n",
    "    epochs = training_config.epochs\n",
    "    learning_rate = training_config.learning_rate\n",
    "    batch_size = training_config.batch_size\n",
    "    num_workers = training_config.num_workers\n",
    "\n",
    "    # Get the ModelConfig for the chosen dataset\n",
    "    if dataset_name not in ALL_MODEL_CONFIGS:\n",
    "        print(f\"Error: Dataset name '{dataset_name}' not found in ALL_MODEL_CONFIGS.\")\n",
    "        print(\"Please ensure 'data_preprocessing.py' defines this dataset name.\")\n",
    "        return\n",
    "\n",
    "    current_dataset_model_config = ALL_MODEL_CONFIGS[dataset_name]\n",
    "    max_frames_spec = current_dataset_model_config.max_frames_spec\n",
    "\n",
    "    # Device configuration\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Determine ViT configuration based on model_size\n",
    "    vit_configs = {\n",
    "        \"ViT_Small\": ViTConfig(\n",
    "            name=\"ViT_Small\",\n",
    "            image_size=(\n",
    "                DataConfig.N_MELS,\n",
    "                current_dataset_model_config.max_frames_spec,\n",
    "            ),\n",
    "            patch_size=(16, 16),\n",
    "            dim=224,  # Embedding dimension\n",
    "            depth=8,  # Number of Transformer blocks\n",
    "            heads=8,  # Number of attention heads\n",
    "            mlp_dim=224 * 4,  # Typically 4x dim\n",
    "        ),\n",
    "        \"ViT_Medium\": ViTConfig(\n",
    "            name=\"ViT_Medium\",\n",
    "            image_size=(\n",
    "                DataConfig.N_MELS,\n",
    "                current_dataset_model_config.max_frames_spec,\n",
    "            ),\n",
    "            patch_size=(16, 16),\n",
    "            dim=256,\n",
    "            depth=12,\n",
    "            heads=8,\n",
    "            mlp_dim=256 * 4,\n",
    "        ),\n",
    "        \"ViT_Large\": ViTConfig(\n",
    "            name=\"ViT_Large\",\n",
    "            image_size=(\n",
    "                DataConfig.N_MELS,\n",
    "                current_dataset_model_config.max_frames_spec,\n",
    "            ),\n",
    "            patch_size=(16, 16),\n",
    "            dim=384,\n",
    "            depth=16,\n",
    "            heads=12,\n",
    "            mlp_dim=384 * 4,\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    if model_size not in vit_configs:\n",
    "        print(\n",
    "            f\"Error: Invalid model_size '{model_size}'. Choose from {list(vit_configs.keys())}\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    vit_config = vit_configs[model_size]\n",
    "    print(f\"Configuring {vit_config.name} model...\")\n",
    "    print(\n",
    "        f\"Image size: {vit_config.image_height}x{vit_config.image_width}, Patch size: {vit_config.patch_height}x{vit_config.patch_width}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Dim: {vit_config.dim}, Depth: {vit_config.depth}, Heads: {vit_config.heads}\"\n",
    "    )\n",
    "\n",
    "    # Dataset paths\n",
    "    base_cache_dir = DataConfig.CACHE_DIR\n",
    "    model_cache_dir = os.path.join(base_cache_dir, dataset_name)\n",
    "\n",
    "    print(f\"Loading data from: {model_cache_dir}\")\n",
    "\n",
    "    # Datasets\n",
    "    train_dataset = AudioDataset(\n",
    "        model_cache_dir, \"train\", DataConfig.N_MELS, max_frames_spec\n",
    "    )\n",
    "    val_dataset = AudioDataset(\n",
    "        model_cache_dir, \"val\", DataConfig.N_MELS, max_frames_spec\n",
    "    )\n",
    "    test_dataset = AudioDataset(\n",
    "        model_cache_dir, \"test\", DataConfig.N_MELS, max_frames_spec\n",
    "    )\n",
    "\n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        collate_fn=custom_collate_fn,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        collate_fn=custom_collate_fn,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        collate_fn=custom_collate_fn,\n",
    "    )\n",
    "\n",
    "    print(f\"Train samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    print(f\"Test samples: {len(test_dataset)}\")\n",
    "    print(f\"Using Batch size: {batch_size}\")\n",
    "\n",
    "    # Initialize model, loss, and optimizer\n",
    "    model = AudioViT(config=vit_config)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Run name for W&B\n",
    "    run_name = f\"{model_size}_{dataset_name}_{datetime.now().strftime('%H%M%S')}\"\n",
    "\n",
    "    # Train the model\n",
    "    trained_model = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        device,\n",
    "        epochs,\n",
    "        run_name,\n",
    "        dataset_name,\n",
    "    )\n",
    "\n",
    "    # Evaluate on test set\n",
    "    print(f\"\\n--- Evaluating {vit_config.name} on Test Set ({dataset_name}) ---\")\n",
    "    test_loss, test_preds, test_labels, test_probs, test_cm = evaluate_model(\n",
    "        trained_model, test_loader, criterion, device, return_cm=True\n",
    "    )\n",
    "\n",
    "    test_acc = accuracy_score(test_labels, test_preds)\n",
    "    test_f1 = f1_score(test_labels, test_preds, average=\"binary\")\n",
    "    test_roc_auc = roc_auc_score(test_labels, test_probs[:, 1])\n",
    "\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"Test F1-score: {test_f1:.4f}\")\n",
    "    print(f\"Test ROC AUC: {test_roc_auc:.4f}\")\n",
    "\n",
    "    # Plot and save confusion matrix\n",
    "    cm_plot_path = plot_confusion_matrix(test_cm, run_name=run_name, save_dir=\"results\")\n",
    "\n",
    "    # Log test metrics to W&B\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"test_loss\": test_loss,\n",
    "            \"test_accuracy\": test_acc,\n",
    "            \"test_f1_score\": test_f1,\n",
    "            \"test_roc_auc\": test_roc_auc,\n",
    "            \"confusion_matrix\": wandb.Image(cm_plot_path),\n",
    "        }\n",
    "    )\n",
    "    wandb.finish()\n",
    "\n",
    "    return trained_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cad270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure training parameters\n",
    "training_params = {\n",
    "    \"model_size\": \"ViT_Small\",\n",
    "    \"dataset_name\": \"vit_balanced_dataset\",\n",
    "    \"epochs\": 20,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_workers\": 8,\n",
    "}\n",
    "\n",
    "# Initialize TrainingConfig with the specified parameters\n",
    "training_config = TrainingConfig(**training_params)\n",
    "\n",
    "# Print training configuration\n",
    "print(\"=== ViT Training Configuration ===\")\n",
    "print(f\"Model Size: {training_config.model_size}\")\n",
    "print(f\"Dataset: {training_config.dataset_name}\")\n",
    "print(f\"Epochs: {training_config.epochs}\")\n",
    "print(f\"Learning Rate: {training_config.learning_rate}\")\n",
    "print(f\"Batch Size: {training_config.batch_size}\")\n",
    "print(f\"Num Workers: {training_config.num_workers}\")\n",
    "print(\"==================================\")\n",
    "\n",
    "# Run training with the configured TrainingConfig\n",
    "trained_model = run_training(training_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8610f3af",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
