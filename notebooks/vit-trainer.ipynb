{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1d2a9d2",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21080510",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T03:34:29.165321Z",
     "iopub.status.busy": "2025-06-09T03:34:29.165161Z",
     "iopub.status.idle": "2025-06-09T03:34:46.607951Z",
     "shell.execute_reply": "2025-06-09T03:34:46.607216Z",
     "shell.execute_reply.started": "2025-06-09T03:34:29.165306Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import wandb\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# WandB login\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "try:\n",
    "    secret_value_0 = user_secrets.get_secret(\"wandb_api_key\")\n",
    "    wandb.login(key=secret_value_0)\n",
    "    print(\"WandB login successful using wandb_api_key.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to login to WandB: {e}. Please ensure WANDB_API_KEY is set.\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24d3162",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c95102c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T03:34:46.609141Z",
     "iopub.status.busy": "2025-06-09T03:34:46.608743Z",
     "iopub.status.idle": "2025-06-09T03:34:46.618540Z",
     "shell.execute_reply": "2025-06-09T03:34:46.617806Z",
     "shell.execute_reply.started": "2025-06-09T03:34:46.609123Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    # Data processing (các tham số này là chung cho dataset, không thay đổi giữa các model)\n",
    "    SEED: int = 42\n",
    "    SR: int = 16000\n",
    "    N_FFT: int = 2048\n",
    "    HOP_LENGTH: int = 512\n",
    "    N_MELS: int = 128\n",
    "    FMIN: float = 0.0\n",
    "    FMAX: float = 8000.0\n",
    "    NUM_TIME_MASKS: int = 2\n",
    "    NUM_FREQ_MASKS: int = 2\n",
    "    TIME_MASK_MAX_WIDTH: int = 30\n",
    "    FREQ_MASK_MAX_WIDTH: int = 15\n",
    "    MASK_REPLACEMENT_VALUE: float = -80.0\n",
    "    NORM_EPSILON: float = 1e-6\n",
    "    LOUDNESS_LUFS: float = -23.0\n",
    "    USE_GLOBAL_NORMALIZATION: bool = True\n",
    "    USE_RANDOM_CROPPING: bool = True\n",
    "    # CHỖ NÀY LÀ ĐƯỜNG DẪN CHUNG CHO CẢ HAI LOẠI MODEL\n",
    "    CACHE_DIR_BASE: str = \"/kaggle/input/vit-3s-dataset\" # Đường dẫn kaggle\n",
    "    # CACHE_DIR_BASE: str = \"F:\\Deepfake-Audio-Detector\\datasets\\final_dataset\"  # Đường dẫn local\n",
    "    DATASET_SUBDIR: str = \"vit_3s_dataset\" # Thư mục con cụ thể của dataset\n",
    "    train_dir: str = \"train\"\n",
    "    val_dir: str = \"val\"\n",
    "    test_dir: str = \"test\"\n",
    "    metadata_file: str = \"kaggle_metadata.csv\" # Metadata kaggle\n",
    "    # metadata_file: str = \"metadata.csv\"  # Metadata local\n",
    "\n",
    "    # Model architecture (các tham số này sẽ khác nhau tùy theo model_size)\n",
    "    img_size: int = 224\n",
    "    patch_size: int = 16\n",
    "    num_classes: int = 2\n",
    "    in_channels: int = 1\n",
    "    dim: int = 128\n",
    "    depth: int = 4\n",
    "    heads: int = 4\n",
    "    mlp_dim: int = 256\n",
    "    dropout: float = 0.1\n",
    "\n",
    "    # Training (có thể là chung hoặc khác nhau tùy theo model)\n",
    "    learning_rate: float = 1e-4\n",
    "    batch_size: int = 32\n",
    "    epochs: int = 20\n",
    "    weight_decay: float = 1e-4\n",
    "    num_workers: int = 4\n",
    "\n",
    "    # Data augmentation\n",
    "    apply_augmentation: bool = True\n",
    "    augmentation_prob: float = 0.5\n",
    "    audio_length_seconds: float = 3.0\n",
    "    overlap_ratio: float = 0.5\n",
    "\n",
    "    # Thuộc tính để lưu trữ tên model và dataset cho mục đích cấu hình và logging\n",
    "    model_size: str = \"\"\n",
    "    dataset_name: str = \"\" # Tên logic của dataset, ví dụ \"vit_3s_dataset\"\n",
    "\n",
    "    def validate(self):\n",
    "        assert self.img_size % self.patch_size == 0, (\"img_size must be divisible by patch_size\")\n",
    "        assert self.dim % self.heads == 0, \"dim must be divisible by heads\"\n",
    "        assert self.learning_rate > 0, \"learning_rate must be positive\"\n",
    "        assert self.batch_size > 0, \"batch_size must be positive\"\n",
    "        assert self.epochs > 0, \"epochs must be positive\"\n",
    "        assert self.num_workers >= 0, \"num_workers must be non-negative\"\n",
    "\n",
    "    # Hàm trợ giúp để tạo đường dẫn cache đầy đủ\n",
    "    def get_full_cache_dir(self):\n",
    "        return os.path.join(self.CACHE_DIR_BASE, self.DATASET_SUBDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f7da85-cdc1-41bc-816c-48ba1714bbd2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T03:34:46.620418Z",
     "iopub.status.busy": "2025-06-09T03:34:46.620214Z",
     "iopub.status.idle": "2025-06-09T03:34:46.634834Z",
     "shell.execute_reply": "2025-06-09T03:34:46.634072Z",
     "shell.execute_reply.started": "2025-06-09T03:34:46.620402Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "BASE_CONFIG = Config()\n",
    "\n",
    "# Lấy tất cả các tham số từ BASE_CONFIG ngoại trừ các tham số kiến trúc và tên model/dataset\n",
    "# mà chúng ta muốn ghi đè riêng cho từng loại model\n",
    "base_params = {\n",
    "    f.name: getattr(BASE_CONFIG, f.name)\n",
    "    for f in BASE_CONFIG.__dataclass_fields__.values()\n",
    "    if f.init and f.name not in ['dim', 'depth', 'heads', 'mlp_dim', 'model_size', 'dataset_name']\n",
    "}\n",
    "\n",
    "ALL_MODEL_CONFIGS = {\n",
    "    \"ViT_Small\": Config(\n",
    "        **base_params, # Giải nén các tham số chung từ BASE_CONFIG\n",
    "        # Ghi đè các tham số cụ thể cho ViT_Small\n",
    "        dim=128,\n",
    "        depth=4,\n",
    "        heads=4,\n",
    "        mlp_dim=256,\n",
    "        model_size=\"ViT_Small\", # Đặt tên model_size\n",
    "        dataset_name=\"vit_3s_dataset\", # Đặt tên dataset logic\n",
    "    ),\n",
    "    \"ViT_Large\": Config(\n",
    "        **base_params, # Giải nén các tham số chung từ BASE_CONFIG\n",
    "        # Ghi đè các tham số cụ thể cho ViT_Large\n",
    "        dim=384,\n",
    "        depth=6,\n",
    "        heads=8,\n",
    "        mlp_dim=768,\n",
    "        model_size=\"ViT_Large\", # Đặt tên model_size\n",
    "        dataset_name=\"vit_3s_dataset\", # Đặt tên dataset logic\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860fb9d9",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35ccef7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T03:34:46.635726Z",
     "iopub.status.busy": "2025-06-09T03:34:46.635495Z",
     "iopub.status.idle": "2025-06-09T03:34:46.649792Z",
     "shell.execute_reply": "2025-06-09T03:34:46.649129Z",
     "shell.execute_reply.started": "2025-06-09T03:34:46.635681Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ViT_Audio(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, num_classes, in_channels, dim, depth, heads, mlp_dim, dropout: float = 0.1): # THÊM dropout VÀO ĐÂY\n",
    "        super().__init__()\n",
    "        assert img_size % patch_size == 0, \"Image dimensions must be divisible by the patch size.\"\n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "        patch_dim = in_channels * patch_size * patch_size\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, patch_dim, kernel_size=patch_size, stride=patch_size),\n",
    "            Rearrange('b c h w -> b (h w) c'),\n",
    "            nn.LayerNorm(patch_dim),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "            nn.LayerNorm(dim),\n",
    "        )\n",
    "\n",
    "        transformer_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=dim,\n",
    "            nhead=heads,\n",
    "            dim_feedforward=mlp_dim,\n",
    "            dropout=dropout, # TRUYỀN dropout VÀO ĐÂY\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(transformer_layer, num_layers=depth)\n",
    "\n",
    "        self.ln = nn.LayerNorm(dim)\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.to_patch_embedding(x)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(b, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.pos_embed # Positional embedding\n",
    "        x = self.transformer(x) # Chú ý rằng PyTorch's TransformerEncoderLayer/Encoder tự xử lý dropout nội bộ\n",
    "\n",
    "        cls_token_final = x[:, 0]\n",
    "        x = self.ln(cls_token_final)\n",
    "        return self.mlp_head(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5885b4f2",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab1d80c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T03:34:46.650593Z",
     "iopub.status.busy": "2025-06-09T03:34:46.650411Z",
     "iopub.status.idle": "2025-06-09T03:34:46.665060Z",
     "shell.execute_reply": "2025-06-09T03:34:46.664257Z",
     "shell.execute_reply.started": "2025-06-09T03:34:46.650578Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    # Đã cập nhật: Loại bỏ 'max_frames_spec' khỏi __init__ vì nó sẽ không được sử dụng để cắt/đệm trực tiếp\n",
    "    def __init__(self, cache_dir: str, set_type: str, n_mels: int, config: Config):\n",
    "        self.cache_path = os.path.join(cache_dir, getattr(config, f\"{set_type}_dir\"))\n",
    "        self.metadata_path = os.path.join(self.cache_path, config.metadata_file)\n",
    "        self.n_mels = n_mels\n",
    "        # self.target_frames = max_frames_spec # Dòng này không còn được sử dụng để cắt/đệm trước\n",
    "        self.training = set_type == \"train\"\n",
    "        self.config = config\n",
    "\n",
    "        if not os.path.exists(self.metadata_path):\n",
    "            raise FileNotFoundError(f\"Metadata file not found: {self.metadata_path}\")\n",
    "        self.metadata = pd.read_csv(self.metadata_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.metadata.iloc[idx]\n",
    "        npy_path = os.path.join(self.cache_path, row[\"npy_path\"])\n",
    "        label = int(row[\"label\"])\n",
    "\n",
    "        try:\n",
    "            if not os.path.exists(npy_path):\n",
    "                raise FileNotFoundError(f\"Spectrogram file not found: {npy_path}\")\n",
    "            spectrogram = np.load(npy_path)\n",
    "            spectrogram = self._preprocess_spectrogram(spectrogram)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {npy_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "        return spectrogram, torch.tensor(label).long()\n",
    "\n",
    "    def _preprocess_spectrogram(self, spec):\n",
    "        if isinstance(spec, np.ndarray):\n",
    "            spec = torch.from_numpy(spec).float()\n",
    "\n",
    "        if spec.ndim == 2:\n",
    "            spec = spec.unsqueeze(0)\n",
    "        elif spec.ndim == 4:\n",
    "            spec = spec.squeeze(0)\n",
    "\n",
    "        # Đã cập nhật: Loại bỏ logic cắt/đệm dựa trên target_frames (max_frames_spec)\n",
    "        # Giờ đây chỉ nội suy trực tiếp đến kích thước img_size x img_size của ViT\n",
    "        if spec.shape[-2:] != (self.config.img_size, self.config.img_size):\n",
    "            spec = F.interpolate(\n",
    "                spec.unsqueeze(0),\n",
    "                size=(self.config.img_size, self.config.img_size),\n",
    "                mode=\"bilinear\",\n",
    "                align_corners=False,\n",
    "            ).squeeze(0)\n",
    "\n",
    "        if self.config.USE_GLOBAL_NORMALIZATION:\n",
    "            mean = spec.mean()\n",
    "            std = spec.std() + self.config.NORM_EPSILON\n",
    "            spec = (spec - mean) / std\n",
    "\n",
    "        if self.training and self.config.apply_augmentation:\n",
    "            # Augmentation chỉ áp dụng sau khi đã nội suy về kích thước cuối cùng\n",
    "            for _ in range(self.config.NUM_FREQ_MASKS):\n",
    "                freq_mask_width = torch.randint(\n",
    "                    0, self.config.FREQ_MASK_MAX_WIDTH, (1,)\n",
    "                ).item()\n",
    "                # Đảm bảo tần số mask không vượt quá kích thước img_size (chiều cao)\n",
    "                freq_start = torch.randint(\n",
    "                    0, max(1, spec.shape[-2] - freq_mask_width), (1,)\n",
    "                ).item()\n",
    "                spec[:, freq_start : freq_start + freq_mask_width, :] = (\n",
    "                    self.config.MASK_REPLACEMENT_VALUE\n",
    "                )\n",
    "            for _ in range(self.config.NUM_TIME_MASKS):\n",
    "                time_mask_width = torch.randint(\n",
    "                    0, self.config.TIME_MASK_MAX_WIDTH, (1,)\n",
    "                ).item()\n",
    "                # Đảm bảo thời gian mask không vượt quá kích thước img_size (chiều rộng)\n",
    "                time_start = torch.randint(\n",
    "                    0, max(1, spec.shape[-1] - time_mask_width), (1,)\n",
    "                ).item()\n",
    "                spec[:, :, time_start : time_start + time_mask_width] = (\n",
    "                    self.config.MASK_REPLACEMENT_VALUE\n",
    "                )\n",
    "\n",
    "        return spec\n",
    "\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    valid_batch = [item for item in batch if item is not None]\n",
    "\n",
    "    if not valid_batch:\n",
    "        print(\"Warning: Empty batch after filtering\")\n",
    "        return torch.empty(0, 1, 224, 224), torch.empty(0, dtype=torch.long)\n",
    "\n",
    "    data_list, label_list = zip(*valid_batch)\n",
    "    expected_shape = (1, 224, 224)\n",
    "    valid_data = []\n",
    "    valid_labels = []\n",
    "\n",
    "    for data, label in zip(data_list, label_list):\n",
    "        if isinstance(data, torch.Tensor) and data.shape == expected_shape:\n",
    "            valid_data.append(data)\n",
    "            valid_labels.append(label)\n",
    "        else:\n",
    "            print(\n",
    "                f\"Warning: Skipping invalid shape {data.shape if hasattr(data, 'shape') else type(data)}\"\n",
    "            )\n",
    "\n",
    "    if not valid_data:\n",
    "        print(\"Warning: No valid data in batch\")\n",
    "        return torch.empty(0, 1, 224, 224), torch.empty(0, dtype=torch.long)\n",
    "\n",
    "    return torch.stack(valid_data, dim=0), torch.stack(valid_labels, dim=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b85fcf2",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a9371a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T03:34:46.666508Z",
     "iopub.status.busy": "2025-06-09T03:34:46.666016Z",
     "iopub.status.idle": "2025-06-09T03:34:46.695250Z",
     "shell.execute_reply": "2025-06-09T03:34:46.694584Z",
     "shell.execute_reply.started": "2025-06-09T03:34:46.666489Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def validate_dataset(dataset, name):\n",
    "    invalid_count = 0\n",
    "    invalid_files = []\n",
    "    for idx in range(len(dataset)):\n",
    "        row = dataset.metadata.iloc[idx]\n",
    "        npy_path = os.path.join(dataset.cache_path, row[\"npy_path\"])\n",
    "        if not os.path.exists(npy_path):\n",
    "            invalid_count += 1\n",
    "            invalid_files.append(npy_path)\n",
    "    if invalid_count > 0:\n",
    "        print(f\"Warning: {invalid_count} invalid samples found in {name} dataset\")\n",
    "        for f in invalid_files[:5]:\n",
    "            print(f\"  - Missing file: {f}\")\n",
    "        if len(invalid_files) > 5:\n",
    "            print(f\"  ... and {len(invalid_files) - 5} more\")\n",
    "    return invalid_count\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model, train_loader, val_loader, optimizer, criterion, device, num_epochs, run_name\n",
    "):\n",
    "    model.to(device)\n",
    "    best_val_f1 = -1\n",
    "    patience = 5\n",
    "    patience_counter = 0\n",
    "    warmup_epochs = 3\n",
    "\n",
    "    # Đã cập nhật: Đảm bảo T_max được tính toán chính xác\n",
    "    # Lịch trình Cosine Annealing cho phần sau của quá trình đào tạo\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=num_epochs - warmup_epochs, eta_min=1e-6\n",
    "    )\n",
    "    # Lịch trình Warmup cho các epoch đầu tiên\n",
    "    warmup_scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "        optimizer,\n",
    "        lr_lambda=lambda epoch: (epoch + 1) / warmup_epochs\n",
    "        if epoch < warmup_epochs\n",
    "        else 1.0,\n",
    "    )\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        batch_count = 0\n",
    "\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        for batch_idx, batch in enumerate(pbar):\n",
    "            if batch is None or len(batch[0]) == 0:\n",
    "                print(f\"Warning: Skipping empty batch at index {batch_idx}\")\n",
    "                continue\n",
    "\n",
    "            data, labels = batch\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            batch_count += 1\n",
    "\n",
    "            pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "        if batch_count == 0:\n",
    "            print(f\"Error: No valid batches in epoch {epoch + 1}\")\n",
    "            continue\n",
    "\n",
    "        if epoch < warmup_epochs:\n",
    "            warmup_scheduler.step()\n",
    "        else:\n",
    "            scheduler.step()\n",
    "\n",
    "        val_loss, val_preds, val_labels, val_probs = evaluate_model(\n",
    "            model, val_loader, criterion, device\n",
    "        )\n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        val_f1 = f1_score(val_labels, val_preds, average=\"binary\")\n",
    "        val_roc_auc = roc_auc_score(val_labels, val_probs[:, 1])\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}: Train Loss: {total_loss / batch_count:.4f}, \"\n",
    "            f\"Val Loss: {val_loss:.4f}, Val F1: {val_f1:.4f}\"\n",
    "        )\n",
    "\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": total_loss / batch_count,\n",
    "                \"val_loss\": val_loss,\n",
    "                \"val_f1\": val_f1,\n",
    "                \"val_accuracy\": val_acc,\n",
    "                \"val_roc_auc\": val_roc_auc,\n",
    "                \"learning_rate\": optimizer.param_groups[0][\"lr\"],\n",
    "                \"warmup_phase\": epoch < warmup_epochs,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            patience_counter = 0\n",
    "            model_save_path = f\"best_model_{run_name}.pth\"\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "                    \"epoch\": epoch,\n",
    "                    \"best_val_f1\": best_val_f1,\n",
    "                },\n",
    "                model_save_path,\n",
    "            )\n",
    "            print(f\"Saved best model with F1: {best_val_f1:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(model, loader, criterion, device, return_cm=False):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader, desc=\"Evaluating\", leave=False)\n",
    "        for batch in pbar:\n",
    "            if batch is None or len(batch[0]) == 0:\n",
    "                continue\n",
    "            data, labels = batch\n",
    "            if -1 in labels.cpu().numpy():\n",
    "                continue\n",
    "\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "    if len(all_labels) < 2:\n",
    "        print(\"Warning: Too few samples for reliable evaluation\")\n",
    "        return (\n",
    "            float(\"inf\"),\n",
    "            [],\n",
    "            [],\n",
    "            np.array([]),\n",
    "            np.zeros((2, 2)) if return_cm else None,\n",
    "        )\n",
    "\n",
    "    avg_loss = total_loss / len(loader) if len(loader) > 0 else 0.0\n",
    "    if return_cm:\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        return avg_loss, all_preds, all_labels, np.array(all_probs), cm\n",
    "    return avg_loss, all_preds, all_labels, np.array(all_probs)\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, run_name, save_dir=\"results\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt=\"d\",\n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=[\"Real\", \"Fake\"],\n",
    "        yticklabels=[\"Real\", \"Fake\"],\n",
    "        ax=ax,\n",
    "    )\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"True\")\n",
    "    ax.set_title(f\"Confusion Matrix - {run_name}\")\n",
    "    cm_plot_path = os.path.join(save_dir, f\"cm_{run_name}.png\")\n",
    "    fig.savefig(cm_plot_path, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    return cm_plot_path\n",
    "\n",
    "\n",
    "def run_training(training_params): # training_params sẽ là dictionary chứa model_size, epochs, v.v.\n",
    "    torch.manual_seed(Config.SEED) # Vẫn dùng Config.SEED chung\n",
    "    np.random.seed(Config.SEED)\n",
    "\n",
    "    model_size = training_params[\"model_size\"]\n",
    "    epochs = training_params[\"epochs\"]\n",
    "    learning_rate = training_params[\"learning_rate\"]\n",
    "    batch_size = training_params[\"batch_size\"]\n",
    "    num_workers = training_params[\"num_workers\"]\n",
    "\n",
    "    # Lấy cấu hình đầy đủ cho model_size cụ thể từ ALL_MODEL_CONFIGS\n",
    "    if model_size not in ALL_MODEL_CONFIGS:\n",
    "        print(f\"Error: Model size '{model_size}' not found in ALL_MODEL_CONFIGS.\")\n",
    "        return\n",
    "\n",
    "    config = ALL_MODEL_CONFIGS[model_size] # config bây giờ chứa tất cả params cho model đó\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Khởi tạo mô hình dựa trên các tham số từ 'config'\n",
    "    model = ViT_Audio(\n",
    "        img_size=config.img_size,\n",
    "        patch_size=config.patch_size,\n",
    "        num_classes=config.num_classes,\n",
    "        in_channels=config.in_channels,\n",
    "        dim=config.dim, # Lấy dim từ config\n",
    "        depth=config.depth, # Lấy depth từ config\n",
    "        heads=config.heads, # Lấy heads từ config\n",
    "        mlp_dim=config.mlp_dim, # Lấy mlp_dim từ config\n",
    "        dropout=config.dropout,\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    # summary(model, input_size=(1, 224, 224)) # Đảm bảo dòng này đã được comment hoặc thay thế\n",
    "    param_count = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Configuring {model_size} model with {param_count:,} parameters...\")\n",
    "\n",
    "    # Sử dụng hàm get_full_cache_dir để tạo đường dẫn dataset\n",
    "    model_cache_dir = config.get_full_cache_dir()\n",
    "    print(f\"Loading data from: {model_cache_dir}\")\n",
    "\n",
    "    # Khởi tạo dataset với 'config' hoàn chỉnh\n",
    "    train_dataset = AudioDataset(model_cache_dir, \"train\", config.N_MELS, config)\n",
    "    val_dataset = AudioDataset(model_cache_dir, \"val\", config.N_MELS, config)\n",
    "    test_dataset = AudioDataset(model_cache_dir, \"test\", config.N_MELS, config)\n",
    "\n",
    "    for dataset, name in [\n",
    "        (train_dataset, \"train\"),\n",
    "        (val_dataset, \"val\"),\n",
    "        (test_dataset, \"test\"),\n",
    "    ]:\n",
    "        invalid_count = validate_dataset(dataset, name)\n",
    "        if invalid_count == len(dataset):\n",
    "            print(f\"Error: All samples in {name} dataset are invalid\")\n",
    "            return\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        collate_fn=custom_collate_fn,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        collate_fn=custom_collate_fn,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        collate_fn=custom_collate_fn,\n",
    "    )\n",
    "\n",
    "    print(f\"Train samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    print(f\"Test samples: {len(test_dataset)}\")\n",
    "    print(f\"Using Batch size: {batch_size}\")\n",
    "\n",
    "    class_counts = np.bincount(\n",
    "        [\n",
    "            train_dataset[i][1].item()\n",
    "            for i in range(len(train_dataset))\n",
    "            if train_dataset[i] is not None\n",
    "        ]\n",
    "    )\n",
    "    if 0 in class_counts:\n",
    "        print(\n",
    "            f\"Error: Class {np.argwhere(class_counts == 0).flatten()} has no samples in training dataset\"\n",
    "        )\n",
    "        return\n",
    "    class_weights = torch.tensor(\n",
    "        [1.0 / max(count, 1e-6) for count in class_counts], dtype=torch.float\n",
    "    ).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), lr=learning_rate, weight_decay=config.weight_decay\n",
    "    )\n",
    "\n",
    "    run_name = f\"{model_size}_{config.dataset_name}_{datetime.now().strftime('%H%M%S')}\"\n",
    "    wandb.init(project=\"audio-deepfake-detection\", name=run_name, config=training_params) # Logging training_params\n",
    "    \n",
    "    trained_model = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        device,\n",
    "        epochs,\n",
    "        run_name,\n",
    "    )\n",
    "\n",
    "    print(f\"\\n--- Evaluating {model_size} on Test Set ({config.dataset_name}) ---\")\n",
    "    test_loss, test_preds, test_labels, test_probs, test_cm = evaluate_model(\n",
    "        trained_model, test_loader, criterion, device, return_cm=True\n",
    "    )\n",
    "\n",
    "    test_acc = accuracy_score(test_labels, test_preds)\n",
    "    test_f1 = f1_score(test_labels, test_preds, average=\"binary\")\n",
    "    test_roc_auc = roc_auc_score(test_labels, test_probs[:, 1])\n",
    "\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"Test F1-score: {test_f1:.4f}\")\n",
    "    print(f\"Test ROC AUC: {test_roc_auc:.4f}\")\n",
    "\n",
    "    cm_plot_path = plot_confusion_matrix(test_cm, run_name=run_name, save_dir=\"results\")\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"test_loss\": test_loss,\n",
    "            \"test_accuracy\": test_acc,\n",
    "            \"test_f1_score\": test_f1,\n",
    "            \"test_roc_auc\": test_roc_auc,\n",
    "            \"confusion_matrix\": wandb.Image(cm_plot_path),\n",
    "        }\n",
    "    )\n",
    "    wandb.finish()\n",
    "\n",
    "    return trained_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6725995",
   "metadata": {},
   "source": [
    "### Define training parameters for ViT models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0954edb",
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-09T03:51:51.252Z",
     "iopub.execute_input": "2025-06-09T03:34:46.696066Z",
     "iopub.status.busy": "2025-06-09T03:34:46.695806Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "training_params_small = {\n",
    "    \"model_size\": \"ViT_Small\", \n",
    "    \"epochs\": 20,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_workers\": 4,\n",
    "}\n",
    "\n",
    "training_params_large = {\n",
    "    \"model_size\": \"ViT_Large\", \n",
    "    \"epochs\": 20,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_workers\": 4,\n",
    "}\n",
    "\n",
    "\n",
    "print(\"=== Training ViT_Small ===\")\n",
    "trained_model_small = run_training(training_params_small)\n",
    "\n",
    "print(\"\\n=== Training ViT_Large ===\")\n",
    "trained_model_large = run_training(training_params_large)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7611148,
     "sourceId": 12090521,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
