{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94e5f835",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "063c9dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_npy_files(BASE_DIR, DATASET, SET_TYPES):\n",
    "    \"\"\"\n",
    "    Inspect .npy files in the dataset and report their shapes.\n",
    "    Check metadata.csv for consistency.\n",
    "    \"\"\"\n",
    "    report = {}\n",
    "    for set_type in SET_TYPES:\n",
    "        split_dir = os.path.join(BASE_DIR, DATASET, set_type)\n",
    "        metadata_path = os.path.join(split_dir, \"metadata.csv\")\n",
    "\n",
    "        if not os.path.exists(metadata_path):\n",
    "            print(f\"Metadata file not found for {set_type}: {metadata_path}\")\n",
    "            report[set_type] = {\"error\": \"Missing metadata\"}\n",
    "            continue\n",
    "\n",
    "        metadata = pd.read_csv(metadata_path)\n",
    "        shapes = []\n",
    "        missing_files = []\n",
    "        invalid_shapes = []\n",
    "\n",
    "        print(f\"\\nInspecting {set_type} set_type...\")\n",
    "        for _, row in tqdm(\n",
    "            metadata.iterrows(), total=len(metadata), desc=f\"Processing {set_type}\"\n",
    "        ):\n",
    "            npy_path = os.path.join(split_dir, row[\"npy_path\"])\n",
    "            if not os.path.exists(npy_path):\n",
    "                missing_files.append(row[\"npy_path\"])\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                spec = np.load(npy_path)\n",
    "                shapes.append(spec.shape)\n",
    "                if spec.shape != (224, 224):\n",
    "                    invalid_shapes.append((row[\"npy_path\"], spec.shape))\n",
    "            except Exception as e:\n",
    "                invalid_shapes.append((row[\"npy_path\"], f\"Error: {e}\"))\n",
    "\n",
    "        report[set_type] = {\n",
    "            \"total_files\": len(metadata),\n",
    "            \"missing_files\": len(missing_files),\n",
    "            \"unique_shapes\": list(set(shapes)),\n",
    "            \"invalid_shapes\": invalid_shapes,\n",
    "            \"sample_shapes\": shapes[:5],  # First 5 shapes for reference\n",
    "        }\n",
    "\n",
    "        print(f\"{set_type} Summary:\")\n",
    "        print(f\"Total files in metadata: {len(metadata)}\")\n",
    "        print(f\"Missing files: {len(missing_files)}\")\n",
    "        print(f\"Unique shapes: {list(set(shapes))}\")\n",
    "        print(f\"Invalid shapes: {len(invalid_shapes)}\")\n",
    "        if invalid_shapes:\n",
    "            print(f\"Sample invalid shapes: {invalid_shapes[:5]}\")\n",
    "\n",
    "    return report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffd4ba86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inspecting train set_type...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train: 100%|██████████| 102896/102896 [01:19<00:00, 1297.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Summary:\n",
      "Total files in metadata: 102896\n",
      "Missing files: 0\n",
      "Unique shapes: [(224, 224)]\n",
      "Invalid shapes: 0\n",
      "\n",
      "Inspecting val set_type...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing val: 100%|██████████| 6996/6996 [00:05<00:00, 1336.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Summary:\n",
      "Total files in metadata: 6996\n",
      "Missing files: 0\n",
      "Unique shapes: [(224, 224)]\n",
      "Invalid shapes: 0\n",
      "\n",
      "Inspecting test set_type...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test: 100%|██████████| 14066/14066 [00:10<00:00, 1311.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Summary:\n",
      "Total files in metadata: 14066\n",
      "Missing files: 0\n",
      "Unique shapes: [(224, 224)]\n",
      "Invalid shapes: 0\n",
      "\n",
      "Inspection report saved to F:\\Deepfake-Audio-Detector\\datasets\\final_dataset\\vit_3s_dataset\\npy_inspection_report.txt\n",
      "\n",
      "Inspecting train set_type...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train: 100%|██████████| 102896/102896 [01:19<00:00, 1300.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Summary:\n",
      "Total files in metadata: 102896\n",
      "Missing files: 0\n",
      "Unique shapes: [(224, 224)]\n",
      "Invalid shapes: 0\n",
      "\n",
      "Inspecting val set_type...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing val: 100%|██████████| 6996/6996 [00:05<00:00, 1244.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Summary:\n",
      "Total files in metadata: 6996\n",
      "Missing files: 0\n",
      "Unique shapes: [(224, 224)]\n",
      "Invalid shapes: 0\n",
      "\n",
      "Inspecting test set_type...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test: 100%|██████████| 14066/14066 [00:11<00:00, 1256.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Summary:\n",
      "Total files in metadata: 14066\n",
      "Missing files: 0\n",
      "Unique shapes: [(224, 224)]\n",
      "Invalid shapes: 0\n",
      "\n",
      "Inspection report saved to F:\\Deepfake-Audio-Detector\\datasets\\final_dataset\\cnn_3s_dataset\\npy_inspection_report.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "BASE_DIR = \"F:\\\\Deepfake-Audio-Detector\\\\datasets\\\\final_dataset\\\\\"  # Your local dataset directory\n",
    "DATASETS = [\"vit_3s_dataset\", \"cnn_3s_dataset\"]\n",
    "SET_TYPES = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "for dataset in DATASETS:\n",
    "    report = inspect_npy_files(BASE_DIR=BASE_DIR, DATASET=dataset, SET_TYPES=SET_TYPES)\n",
    "    with open(os.path.join(BASE_DIR, dataset, \"npy_inspection_report.txt\"), \"w\") as f:\n",
    "        for split, info in report.items():\n",
    "            f.write(f\"\\n{split} Split:\\n\")\n",
    "            for key, value in info.items():\n",
    "                f.write(f\"{key}: {value}\\n\")\n",
    "    print(f\"\\nInspection report saved to {os.path.join(BASE_DIR, dataset, 'npy_inspection_report.txt')}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
