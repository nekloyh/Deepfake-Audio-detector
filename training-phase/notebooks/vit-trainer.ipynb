{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11e53cbf",
   "metadata": {
    "papermill": {
     "duration": 0.003215,
     "end_time": "2025-06-09T03:52:23.602926",
     "exception": false,
     "start_time": "2025-06-09T03:52:23.599711",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a4dbaac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T03:52:23.609403Z",
     "iopub.status.busy": "2025-06-09T03:52:23.608827Z",
     "iopub.status.idle": "2025-06-09T03:52:38.457779Z",
     "shell.execute_reply": "2025-06-09T03:52:38.457056Z"
    },
    "papermill": {
     "duration": 14.853214,
     "end_time": "2025-06-09T03:52:38.458925",
     "exception": false,
     "start_time": "2025-06-09T03:52:23.605711",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnekloyh\u001b[0m (\u001b[33mnekloyh-none\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WandB login successful using wandb_api_key.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import wandb\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# WandB login\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "try:\n",
    "    secret_value_0 = user_secrets.get_secret(\"wandb_api_key\")\n",
    "    wandb.login(key=secret_value_0)\n",
    "    print(\"WandB login successful using wandb_api_key.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to login to WandB: {e}. Please ensure WANDB_API_KEY is set.\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4290ad74",
   "metadata": {
    "papermill": {
     "duration": 0.002605,
     "end_time": "2025-06-09T03:52:38.464563",
     "exception": false,
     "start_time": "2025-06-09T03:52:38.461958",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e6f5b1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T03:52:38.471165Z",
     "iopub.status.busy": "2025-06-09T03:52:38.470835Z",
     "iopub.status.idle": "2025-06-09T03:52:38.480883Z",
     "shell.execute_reply": "2025-06-09T03:52:38.480209Z"
    },
    "papermill": {
     "duration": 0.014729,
     "end_time": "2025-06-09T03:52:38.482048",
     "exception": false,
     "start_time": "2025-06-09T03:52:38.467319",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    # Data processing (c√°c tham s·ªë n√†y l√† chung cho dataset, kh√¥ng thay ƒë·ªïi gi·ªØa c√°c model)\n",
    "    SEED: int = 42\n",
    "    SR: int = 16000\n",
    "    N_FFT: int = 2048\n",
    "    HOP_LENGTH: int = 512\n",
    "    N_MELS: int = 128\n",
    "    FMIN: float = 0.0\n",
    "    FMAX: float = 8000.0\n",
    "    NUM_TIME_MASKS: int = 2\n",
    "    NUM_FREQ_MASKS: int = 2\n",
    "    TIME_MASK_MAX_WIDTH: int = 30\n",
    "    FREQ_MASK_MAX_WIDTH: int = 15\n",
    "    MASK_REPLACEMENT_VALUE: float = -80.0\n",
    "    NORM_EPSILON: float = 1e-6\n",
    "    LOUDNESS_LUFS: float = -23.0\n",
    "    USE_GLOBAL_NORMALIZATION: bool = True\n",
    "    USE_RANDOM_CROPPING: bool = True\n",
    "    # CH·ªñ N√ÄY L√Ä ƒê∆Ø·ªúNG D·∫™N CHUNG CHO C·∫¢ HAI LO·∫†I MODEL\n",
    "    CACHE_DIR_BASE: str = \"/kaggle/input/vit-3s-dataset\" # ƒê∆∞·ªùng d·∫´n g·ªëc\n",
    "    DATASET_SUBDIR: str = \"vit_3s_dataset\" # Th∆∞ m·ª•c con c·ª• th·ªÉ c·ªßa dataset\n",
    "    train_dir: str = \"train\"\n",
    "    val_dir: str = \"val\"\n",
    "    test_dir: str = \"test\"\n",
    "    metadata_file: str = \"kaggle_metadata.csv\"\n",
    "\n",
    "    # Model architecture (c√°c tham s·ªë n√†y s·∫Ω kh√°c nhau t√πy theo model_size)\n",
    "    img_size: int = 224\n",
    "    patch_size: int = 16\n",
    "    num_classes: int = 2\n",
    "    in_channels: int = 1\n",
    "    dim: int = 128\n",
    "    depth: int = 4\n",
    "    heads: int = 4\n",
    "    mlp_dim: int = 256\n",
    "    dropout: float = 0.1\n",
    "\n",
    "    # Training (c√≥ th·ªÉ l√† chung ho·∫∑c kh√°c nhau t√πy theo model)\n",
    "    learning_rate: float = 1e-4\n",
    "    batch_size: int = 32\n",
    "    epochs: int = 20\n",
    "    weight_decay: float = 1e-4\n",
    "    num_workers: int = 4\n",
    "\n",
    "    # Data augmentation\n",
    "    apply_augmentation: bool = True\n",
    "    augmentation_prob: float = 0.5\n",
    "    audio_length_seconds: float = 3.0\n",
    "    overlap_ratio: float = 0.5\n",
    "\n",
    "    # Thu·ªôc t√≠nh ƒë·ªÉ l∆∞u tr·ªØ t√™n model v√† dataset cho m·ª•c ƒë√≠ch c·∫•u h√¨nh v√† logging\n",
    "    model_size: str = \"\"\n",
    "    dataset_name: str = \"\" # T√™n logic c·ªßa dataset, v√≠ d·ª• \"vit_3s_dataset\"\n",
    "\n",
    "    def validate(self):\n",
    "        assert self.img_size % self.patch_size == 0, (\"img_size must be divisible by patch_size\")\n",
    "        assert self.dim % self.heads == 0, \"dim must be divisible by heads\"\n",
    "        assert self.learning_rate > 0, \"learning_rate must be positive\"\n",
    "        assert self.batch_size > 0, \"batch_size must be positive\"\n",
    "        assert self.epochs > 0, \"epochs must be positive\"\n",
    "        assert self.num_workers >= 0, \"num_workers must be non-negative\"\n",
    "\n",
    "    # H√†m tr·ª£ gi√∫p ƒë·ªÉ t·∫°o ƒë∆∞·ªùng d·∫´n cache ƒë·∫ßy ƒë·ªß\n",
    "    def get_full_cache_dir(self):\n",
    "        return os.path.join(self.CACHE_DIR_BASE, self.DATASET_SUBDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2432ef7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T03:52:38.488468Z",
     "iopub.status.busy": "2025-06-09T03:52:38.487926Z",
     "iopub.status.idle": "2025-06-09T03:52:38.492715Z",
     "shell.execute_reply": "2025-06-09T03:52:38.492048Z"
    },
    "papermill": {
     "duration": 0.009067,
     "end_time": "2025-06-09T03:52:38.493783",
     "exception": false,
     "start_time": "2025-06-09T03:52:38.484716",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "BASE_CONFIG = Config()\n",
    "\n",
    "# L·∫•y t·∫•t c·∫£ c√°c tham s·ªë t·ª´ BASE_CONFIG ngo·∫°i tr·ª´ c√°c tham s·ªë ki·∫øn tr√∫c v√† t√™n model/dataset\n",
    "# m√† ch√∫ng ta mu·ªën ghi ƒë√® ri√™ng cho t·ª´ng lo·∫°i model\n",
    "base_params = {\n",
    "    f.name: getattr(BASE_CONFIG, f.name)\n",
    "    for f in BASE_CONFIG.__dataclass_fields__.values()\n",
    "    if f.init and f.name not in ['dim', 'depth', 'heads', 'mlp_dim', 'model_size', 'dataset_name']\n",
    "}\n",
    "\n",
    "ALL_MODEL_CONFIGS = {\n",
    "    \"ViT_Small\": Config(\n",
    "        **base_params, # Gi·∫£i n√©n c√°c tham s·ªë chung t·ª´ BASE_CONFIG\n",
    "        # Ghi ƒë√® c√°c tham s·ªë c·ª• th·ªÉ cho ViT_Small\n",
    "        dim=128,\n",
    "        depth=4,\n",
    "        heads=4,\n",
    "        mlp_dim=256,\n",
    "        model_size=\"ViT_Small\", # ƒê·∫∑t t√™n model_size\n",
    "        dataset_name=\"vit_3s_dataset\", # ƒê·∫∑t t√™n dataset logic\n",
    "    ),\n",
    "    \"ViT_Large\": Config(\n",
    "        **base_params, # Gi·∫£i n√©n c√°c tham s·ªë chung t·ª´ BASE_CONFIG\n",
    "        # Ghi ƒë√® c√°c tham s·ªë c·ª• th·ªÉ cho ViT_Large\n",
    "        dim=384,\n",
    "        depth=6,\n",
    "        heads=8,\n",
    "        mlp_dim=768,\n",
    "        model_size=\"ViT_Large\", # ƒê·∫∑t t√™n model_size\n",
    "        dataset_name=\"vit_3s_dataset\", # ƒê·∫∑t t√™n dataset logic\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653d47ff",
   "metadata": {
    "papermill": {
     "duration": 0.00257,
     "end_time": "2025-06-09T03:52:38.499026",
     "exception": false,
     "start_time": "2025-06-09T03:52:38.496456",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "530d9478",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T03:52:38.505299Z",
     "iopub.status.busy": "2025-06-09T03:52:38.504751Z",
     "iopub.status.idle": "2025-06-09T03:52:38.511491Z",
     "shell.execute_reply": "2025-06-09T03:52:38.510962Z"
    },
    "papermill": {
     "duration": 0.010847,
     "end_time": "2025-06-09T03:52:38.512483",
     "exception": false,
     "start_time": "2025-06-09T03:52:38.501636",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ViT_Audio(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, num_classes, in_channels, dim, depth, heads, mlp_dim, dropout: float = 0.1): # TH√äM dropout V√ÄO ƒê√ÇY\n",
    "        super().__init__()\n",
    "        assert img_size % patch_size == 0, \"Image dimensions must be divisible by the patch size.\"\n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "        patch_dim = in_channels * patch_size * patch_size\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, patch_dim, kernel_size=patch_size, stride=patch_size),\n",
    "            Rearrange('b c h w -> b (h w) c'),\n",
    "            nn.LayerNorm(patch_dim),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "            nn.LayerNorm(dim),\n",
    "        )\n",
    "\n",
    "        transformer_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=dim,\n",
    "            nhead=heads,\n",
    "            dim_feedforward=mlp_dim,\n",
    "            dropout=dropout, # TRUY·ªÄN dropout V√ÄO ƒê√ÇY\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(transformer_layer, num_layers=depth)\n",
    "\n",
    "        self.ln = nn.LayerNorm(dim)\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.to_patch_embedding(x)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(b, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.pos_embed # Positional embedding\n",
    "        x = self.transformer(x) # Ch√∫ √Ω r·∫±ng PyTorch's TransformerEncoderLayer/Encoder t·ª± x·ª≠ l√Ω dropout n·ªôi b·ªô\n",
    "\n",
    "        cls_token_final = x[:, 0]\n",
    "        x = self.ln(cls_token_final)\n",
    "        return self.mlp_head(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25e0f1f",
   "metadata": {
    "papermill": {
     "duration": 0.00263,
     "end_time": "2025-06-09T03:52:38.517813",
     "exception": false,
     "start_time": "2025-06-09T03:52:38.515183",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60a52e64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T03:52:38.524238Z",
     "iopub.status.busy": "2025-06-09T03:52:38.523852Z",
     "iopub.status.idle": "2025-06-09T03:52:38.535874Z",
     "shell.execute_reply": "2025-06-09T03:52:38.535310Z"
    },
    "papermill": {
     "duration": 0.016306,
     "end_time": "2025-06-09T03:52:38.536820",
     "exception": false,
     "start_time": "2025-06-09T03:52:38.520514",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    # ƒê√£ c·∫≠p nh·∫≠t: Lo·∫°i b·ªè 'max_frames_spec' kh·ªèi __init__ v√¨ n√≥ s·∫Ω kh√¥ng ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ c·∫Øt/ƒë·ªám tr·ª±c ti·∫øp\n",
    "    def __init__(self, cache_dir: str, set_type: str, n_mels: int, config: Config):\n",
    "        self.cache_path = os.path.join(cache_dir, getattr(config, f\"{set_type}_dir\"))\n",
    "        self.metadata_path = os.path.join(self.cache_path, config.metadata_file)\n",
    "        self.n_mels = n_mels\n",
    "        # self.target_frames = max_frames_spec # D√≤ng n√†y kh√¥ng c√≤n ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ c·∫Øt/ƒë·ªám tr∆∞·ªõc\n",
    "        self.training = set_type == \"train\"\n",
    "        self.config = config\n",
    "\n",
    "        if not os.path.exists(self.metadata_path):\n",
    "            raise FileNotFoundError(f\"Metadata file not found: {self.metadata_path}\")\n",
    "        self.metadata = pd.read_csv(self.metadata_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.metadata.iloc[idx]\n",
    "        npy_path = os.path.join(self.cache_path, row[\"npy_path\"])\n",
    "        label = int(row[\"label\"])\n",
    "\n",
    "        try:\n",
    "            if not os.path.exists(npy_path):\n",
    "                raise FileNotFoundError(f\"Spectrogram file not found: {npy_path}\")\n",
    "            spectrogram = np.load(npy_path)\n",
    "            spectrogram = self._preprocess_spectrogram(spectrogram)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {npy_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "        return spectrogram, torch.tensor(label).long()\n",
    "\n",
    "    def _preprocess_spectrogram(self, spec):\n",
    "        if isinstance(spec, np.ndarray):\n",
    "            spec = torch.from_numpy(spec).float()\n",
    "\n",
    "        if spec.ndim == 2:\n",
    "            spec = spec.unsqueeze(0)\n",
    "        elif spec.ndim == 4:\n",
    "            spec = spec.squeeze(0)\n",
    "\n",
    "        # ƒê√£ c·∫≠p nh·∫≠t: Lo·∫°i b·ªè logic c·∫Øt/ƒë·ªám d·ª±a tr√™n target_frames (max_frames_spec)\n",
    "        # Gi·ªù ƒë√¢y ch·ªâ n·ªôi suy tr·ª±c ti·∫øp ƒë·∫øn k√≠ch th∆∞·ªõc img_size x img_size c·ªßa ViT\n",
    "        if spec.shape[-2:] != (self.config.img_size, self.config.img_size):\n",
    "            spec = F.interpolate(\n",
    "                spec.unsqueeze(0),\n",
    "                size=(self.config.img_size, self.config.img_size),\n",
    "                mode=\"bilinear\",\n",
    "                align_corners=False,\n",
    "            ).squeeze(0)\n",
    "\n",
    "        if self.config.USE_GLOBAL_NORMALIZATION:\n",
    "            mean = spec.mean()\n",
    "            std = spec.std() + self.config.NORM_EPSILON\n",
    "            spec = (spec - mean) / std\n",
    "\n",
    "        if self.training and self.config.apply_augmentation:\n",
    "            # Augmentation ch·ªâ √°p d·ª•ng sau khi ƒë√£ n·ªôi suy v·ªÅ k√≠ch th∆∞·ªõc cu·ªëi c√πng\n",
    "            for _ in range(self.config.NUM_FREQ_MASKS):\n",
    "                freq_mask_width = torch.randint(\n",
    "                    0, self.config.FREQ_MASK_MAX_WIDTH, (1,)\n",
    "                ).item()\n",
    "                # ƒê·∫£m b·∫£o t·∫ßn s·ªë mask kh√¥ng v∆∞·ª£t qu√° k√≠ch th∆∞·ªõc img_size (chi·ªÅu cao)\n",
    "                freq_start = torch.randint(\n",
    "                    0, max(1, spec.shape[-2] - freq_mask_width), (1,)\n",
    "                ).item()\n",
    "                spec[:, freq_start : freq_start + freq_mask_width, :] = (\n",
    "                    self.config.MASK_REPLACEMENT_VALUE\n",
    "                )\n",
    "            for _ in range(self.config.NUM_TIME_MASKS):\n",
    "                time_mask_width = torch.randint(\n",
    "                    0, self.config.TIME_MASK_MAX_WIDTH, (1,)\n",
    "                ).item()\n",
    "                # ƒê·∫£m b·∫£o th·ªùi gian mask kh√¥ng v∆∞·ª£t qu√° k√≠ch th∆∞·ªõc img_size (chi·ªÅu r·ªông)\n",
    "                time_start = torch.randint(\n",
    "                    0, max(1, spec.shape[-1] - time_mask_width), (1,)\n",
    "                ).item()\n",
    "                spec[:, :, time_start : time_start + time_mask_width] = (\n",
    "                    self.config.MASK_REPLACEMENT_VALUE\n",
    "                )\n",
    "\n",
    "        return spec\n",
    "\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    valid_batch = [item for item in batch if item is not None]\n",
    "\n",
    "    if not valid_batch:\n",
    "        print(\"Warning: Empty batch after filtering\")\n",
    "        return torch.empty(0, 1, 224, 224), torch.empty(0, dtype=torch.long)\n",
    "\n",
    "    data_list, label_list = zip(*valid_batch)\n",
    "    expected_shape = (1, 224, 224)\n",
    "    valid_data = []\n",
    "    valid_labels = []\n",
    "\n",
    "    for data, label in zip(data_list, label_list):\n",
    "        if isinstance(data, torch.Tensor) and data.shape == expected_shape:\n",
    "            valid_data.append(data)\n",
    "            valid_labels.append(label)\n",
    "        else:\n",
    "            print(\n",
    "                f\"Warning: Skipping invalid shape {data.shape if hasattr(data, 'shape') else type(data)}\"\n",
    "            )\n",
    "\n",
    "    if not valid_data:\n",
    "        print(\"Warning: No valid data in batch\")\n",
    "        return torch.empty(0, 1, 224, 224), torch.empty(0, dtype=torch.long)\n",
    "\n",
    "    return torch.stack(valid_data, dim=0), torch.stack(valid_labels, dim=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf38441",
   "metadata": {
    "papermill": {
     "duration": 0.002508,
     "end_time": "2025-06-09T03:52:38.541975",
     "exception": false,
     "start_time": "2025-06-09T03:52:38.539467",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "976f7449",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T03:52:38.548605Z",
     "iopub.status.busy": "2025-06-09T03:52:38.548429Z",
     "iopub.status.idle": "2025-06-09T03:52:38.572700Z",
     "shell.execute_reply": "2025-06-09T03:52:38.572169Z"
    },
    "papermill": {
     "duration": 0.028975,
     "end_time": "2025-06-09T03:52:38.573708",
     "exception": false,
     "start_time": "2025-06-09T03:52:38.544733",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validate_dataset(dataset, name):\n",
    "    invalid_count = 0\n",
    "    invalid_files = []\n",
    "    for idx in range(len(dataset)):\n",
    "        row = dataset.metadata.iloc[idx]\n",
    "        npy_path = os.path.join(dataset.cache_path, row[\"npy_path\"])\n",
    "        if not os.path.exists(npy_path):\n",
    "            invalid_count += 1\n",
    "            invalid_files.append(npy_path)\n",
    "    if invalid_count > 0:\n",
    "        print(f\"Warning: {invalid_count} invalid samples found in {name} dataset\")\n",
    "        for f in invalid_files[:5]:\n",
    "            print(f\"  - Missing file: {f}\")\n",
    "        if len(invalid_files) > 5:\n",
    "            print(f\"  ... and {len(invalid_files) - 5} more\")\n",
    "    return invalid_count\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model, train_loader, val_loader, optimizer, criterion, device, num_epochs, run_name\n",
    "):\n",
    "    model.to(device)\n",
    "    best_val_f1 = -1\n",
    "    patience = 5\n",
    "    patience_counter = 0\n",
    "    warmup_epochs = 3\n",
    "\n",
    "    # ƒê√£ c·∫≠p nh·∫≠t: ƒê·∫£m b·∫£o T_max ƒë∆∞·ª£c t√≠nh to√°n ch√≠nh x√°c\n",
    "    # L·ªãch tr√¨nh Cosine Annealing cho ph·∫ßn sau c·ªßa qu√° tr√¨nh ƒë√†o t·∫°o\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=num_epochs - warmup_epochs, eta_min=1e-6\n",
    "    )\n",
    "    # L·ªãch tr√¨nh Warmup cho c√°c epoch ƒë·∫ßu ti√™n\n",
    "    warmup_scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "        optimizer,\n",
    "        lr_lambda=lambda epoch: (epoch + 1) / warmup_epochs\n",
    "        if epoch < warmup_epochs\n",
    "        else 1.0,\n",
    "    )\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        batch_count = 0\n",
    "\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        for batch_idx, batch in enumerate(pbar):\n",
    "            if batch is None or len(batch[0]) == 0:\n",
    "                print(f\"Warning: Skipping empty batch at index {batch_idx}\")\n",
    "                continue\n",
    "\n",
    "            data, labels = batch\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            batch_count += 1\n",
    "\n",
    "            pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "        if batch_count == 0:\n",
    "            print(f\"Error: No valid batches in epoch {epoch + 1}\")\n",
    "            continue\n",
    "\n",
    "        if epoch < warmup_epochs:\n",
    "            warmup_scheduler.step()\n",
    "        else:\n",
    "            scheduler.step()\n",
    "\n",
    "        val_loss, val_preds, val_labels, val_probs = evaluate_model(\n",
    "            model, val_loader, criterion, device\n",
    "        )\n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        val_f1 = f1_score(val_labels, val_preds, average=\"binary\")\n",
    "        val_roc_auc = roc_auc_score(val_labels, val_probs[:, 1])\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}: Train Loss: {total_loss / batch_count:.4f}, \"\n",
    "            f\"Val Loss: {val_loss:.4f}, Val F1: {val_f1:.4f}\"\n",
    "        )\n",
    "\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": total_loss / batch_count,\n",
    "                \"val_loss\": val_loss,\n",
    "                \"val_f1\": val_f1,\n",
    "                \"val_accuracy\": val_acc,\n",
    "                \"val_roc_auc\": val_roc_auc,\n",
    "                \"learning_rate\": optimizer.param_groups[0][\"lr\"],\n",
    "                \"warmup_phase\": epoch < warmup_epochs,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            patience_counter = 0\n",
    "            model_save_path = f\"best_model_{run_name}.pth\"\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "                    \"epoch\": epoch,\n",
    "                    \"best_val_f1\": best_val_f1,\n",
    "                },\n",
    "                model_save_path,\n",
    "            )\n",
    "            print(f\"Saved best model with F1: {best_val_f1:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(model, loader, criterion, device, return_cm=False):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader, desc=\"Evaluating\", leave=False)\n",
    "        for batch in pbar:\n",
    "            if batch is None or len(batch[0]) == 0:\n",
    "                continue\n",
    "            data, labels = batch\n",
    "            if -1 in labels.cpu().numpy():\n",
    "                continue\n",
    "\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "    if len(all_labels) < 2:\n",
    "        print(\"Warning: Too few samples for reliable evaluation\")\n",
    "        return (\n",
    "            float(\"inf\"),\n",
    "            [],\n",
    "            [],\n",
    "            np.array([]),\n",
    "            np.zeros((2, 2)) if return_cm else None,\n",
    "        )\n",
    "\n",
    "    avg_loss = total_loss / len(loader) if len(loader) > 0 else 0.0\n",
    "    if return_cm:\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        return avg_loss, all_preds, all_labels, np.array(all_probs), cm\n",
    "    return avg_loss, all_preds, all_labels, np.array(all_probs)\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, run_name, save_dir=\"results\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt=\"d\",\n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=[\"Real\", \"Fake\"],\n",
    "        yticklabels=[\"Real\", \"Fake\"],\n",
    "        ax=ax,\n",
    "    )\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"True\")\n",
    "    ax.set_title(f\"Confusion Matrix - {run_name}\")\n",
    "    cm_plot_path = os.path.join(save_dir, f\"cm_{run_name}.png\")\n",
    "    fig.savefig(cm_plot_path, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    return cm_plot_path\n",
    "\n",
    "\n",
    "def run_training(training_params): # training_params s·∫Ω l√† dictionary ch·ª©a model_size, epochs, v.v.\n",
    "    torch.manual_seed(Config.SEED) # V·∫´n d√πng Config.SEED chung\n",
    "    np.random.seed(Config.SEED)\n",
    "\n",
    "    model_size = training_params[\"model_size\"]\n",
    "    epochs = training_params[\"epochs\"]\n",
    "    learning_rate = training_params[\"learning_rate\"]\n",
    "    batch_size = training_params[\"batch_size\"]\n",
    "    num_workers = training_params[\"num_workers\"]\n",
    "\n",
    "    # L·∫•y c·∫•u h√¨nh ƒë·∫ßy ƒë·ªß cho model_size c·ª• th·ªÉ t·ª´ ALL_MODEL_CONFIGS\n",
    "    if model_size not in ALL_MODEL_CONFIGS:\n",
    "        print(f\"Error: Model size '{model_size}' not found in ALL_MODEL_CONFIGS.\")\n",
    "        return\n",
    "\n",
    "    config = ALL_MODEL_CONFIGS[model_size] # config b√¢y gi·ªù ch·ª©a t·∫•t c·∫£ params cho model ƒë√≥\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Kh·ªüi t·∫°o m√¥ h√¨nh d·ª±a tr√™n c√°c tham s·ªë t·ª´ 'config'\n",
    "    model = ViT_Audio(\n",
    "        img_size=config.img_size,\n",
    "        patch_size=config.patch_size,\n",
    "        num_classes=config.num_classes,\n",
    "        in_channels=config.in_channels,\n",
    "        dim=config.dim, # L·∫•y dim t·ª´ config\n",
    "        depth=config.depth, # L·∫•y depth t·ª´ config\n",
    "        heads=config.heads, # L·∫•y heads t·ª´ config\n",
    "        mlp_dim=config.mlp_dim, # L·∫•y mlp_dim t·ª´ config\n",
    "        dropout=config.dropout,\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    # summary(model, input_size=(1, 224, 224)) # ƒê·∫£m b·∫£o d√≤ng n√†y ƒë√£ ƒë∆∞·ª£c comment ho·∫∑c thay th·∫ø\n",
    "    param_count = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Configuring {model_size} model with {param_count:,} parameters...\")\n",
    "\n",
    "    # S·ª≠ d·ª•ng h√†m get_full_cache_dir ƒë·ªÉ t·∫°o ƒë∆∞·ªùng d·∫´n dataset\n",
    "    model_cache_dir = config.get_full_cache_dir()\n",
    "    print(f\"Loading data from: {model_cache_dir}\")\n",
    "\n",
    "    # Kh·ªüi t·∫°o dataset v·ªõi 'config' ho√†n ch·ªânh\n",
    "    train_dataset = AudioDataset(model_cache_dir, \"train\", config.N_MELS, config)\n",
    "    val_dataset = AudioDataset(model_cache_dir, \"val\", config.N_MELS, config)\n",
    "    test_dataset = AudioDataset(model_cache_dir, \"test\", config.N_MELS, config)\n",
    "\n",
    "    for dataset, name in [\n",
    "        (train_dataset, \"train\"),\n",
    "        (val_dataset, \"val\"),\n",
    "        (test_dataset, \"test\"),\n",
    "    ]:\n",
    "        invalid_count = validate_dataset(dataset, name)\n",
    "        if invalid_count == len(dataset):\n",
    "            print(f\"Error: All samples in {name} dataset are invalid\")\n",
    "            return\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        collate_fn=custom_collate_fn,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        collate_fn=custom_collate_fn,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        collate_fn=custom_collate_fn,\n",
    "    )\n",
    "\n",
    "    print(f\"Train samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    print(f\"Test samples: {len(test_dataset)}\")\n",
    "    print(f\"Using Batch size: {batch_size}\")\n",
    "\n",
    "    class_counts = np.bincount(\n",
    "        [\n",
    "            train_dataset[i][1].item()\n",
    "            for i in range(len(train_dataset))\n",
    "            if train_dataset[i] is not None\n",
    "        ]\n",
    "    )\n",
    "    if 0 in class_counts:\n",
    "        print(\n",
    "            f\"Error: Class {np.argwhere(class_counts == 0).flatten()} has no samples in training dataset\"\n",
    "        )\n",
    "        return\n",
    "    class_weights = torch.tensor(\n",
    "        [1.0 / max(count, 1e-6) for count in class_counts], dtype=torch.float\n",
    "    ).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), lr=learning_rate, weight_decay=config.weight_decay\n",
    "    )\n",
    "\n",
    "    run_name = f\"{model_size}_{config.dataset_name}_{datetime.now().strftime('%H%M%S')}\"\n",
    "    wandb.init(project=\"audio-deepfake-detection\", name=run_name, config=training_params) # Logging training_params\n",
    "    \n",
    "    trained_model = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        device,\n",
    "        epochs,\n",
    "        run_name,\n",
    "    )\n",
    "\n",
    "    print(f\"\\n--- Evaluating {model_size} on Test Set ({config.dataset_name}) ---\")\n",
    "    test_loss, test_preds, test_labels, test_probs, test_cm = evaluate_model(\n",
    "        trained_model, test_loader, criterion, device, return_cm=True\n",
    "    )\n",
    "\n",
    "    test_acc = accuracy_score(test_labels, test_preds)\n",
    "    test_f1 = f1_score(test_labels, test_preds, average=\"binary\")\n",
    "    test_roc_auc = roc_auc_score(test_labels, test_probs[:, 1])\n",
    "\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"Test F1-score: {test_f1:.4f}\")\n",
    "    print(f\"Test ROC AUC: {test_roc_auc:.4f}\")\n",
    "\n",
    "    cm_plot_path = plot_confusion_matrix(test_cm, run_name=run_name, save_dir=\"results\")\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"test_loss\": test_loss,\n",
    "            \"test_accuracy\": test_acc,\n",
    "            \"test_f1_score\": test_f1,\n",
    "            \"test_roc_auc\": test_roc_auc,\n",
    "            \"confusion_matrix\": wandb.Image(cm_plot_path),\n",
    "        }\n",
    "    )\n",
    "    wandb.finish()\n",
    "\n",
    "    return trained_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874a8a11",
   "metadata": {
    "papermill": {
     "duration": 0.00249,
     "end_time": "2025-06-09T03:52:38.578828",
     "exception": false,
     "start_time": "2025-06-09T03:52:38.576338",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb241f5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T03:52:38.584856Z",
     "iopub.status.busy": "2025-06-09T03:52:38.584634Z",
     "iopub.status.idle": "2025-06-09T07:41:20.683933Z",
     "shell.execute_reply": "2025-06-09T07:41:20.683097Z"
    },
    "papermill": {
     "duration": 13722.104024,
     "end_time": "2025-06-09T07:41:20.685458",
     "exception": false,
     "start_time": "2025-06-09T03:52:38.581434",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training ViT_Small ===\n",
      "Using device: cuda\n",
      "Configuring ViT_Small model with 655,490 parameters...\n",
      "Loading data from: /kaggle/input/vit-3s-dataset/vit_3s_dataset\n",
      "Train samples: 102896\n",
      "Validation samples: 6996\n",
      "Test samples: 14066\n",
      "Using Batch size: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250609_040441-docydruq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mViT_Small_vit_3s_dataset_040441\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/nekloyh-none/audio-deepfake-detection\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/nekloyh-none/audio-deepfake-detection/runs/docydruq\u001b[0m\n",
      "Epoch 1/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3216/3216 [01:43<00:00, 30.93it/s, loss=0.2755]\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.4096, Val Loss: 0.2894, Val F1: 0.8710\n",
      "Saved best model with F1: 0.8710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3216/3216 [01:42<00:00, 31.34it/s, loss=0.2086]\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 0.3297, Val Loss: 0.2893, Val F1: 0.8667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3216/3216 [01:41<00:00, 31.60it/s, loss=0.2333]\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 0.2992, Val Loss: 0.2051, Val F1: 0.9032\n",
      "Saved best model with F1: 0.9032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3216/3216 [01:41<00:00, 31.60it/s, loss=0.2517]\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 0.2735, Val Loss: 0.1591, Val F1: 0.9334\n",
      "Saved best model with F1: 0.9334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3216/3216 [01:41<00:00, 31.60it/s, loss=0.2979]\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss: 0.2575, Val Loss: 0.1482, Val F1: 0.9356\n",
      "Saved best model with F1: 0.9356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3216/3216 [01:41<00:00, 31.56it/s, loss=0.3246]\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss: 0.2427, Val Loss: 0.1283, Val F1: 0.9458\n",
      "Saved best model with F1: 0.9458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3216/3216 [01:42<00:00, 31.33it/s, loss=0.2582]\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss: 0.2305, Val Loss: 0.1239, Val F1: 0.9543\n",
      "Saved best model with F1: 0.9543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3216/3216 [01:42<00:00, 31.46it/s, loss=0.2188]\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss: 0.2223, Val Loss: 0.1090, Val F1: 0.9595\n",
      "Saved best model with F1: 0.9595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3216/3216 [01:42<00:00, 31.51it/s, loss=0.3748]\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss: 0.2103, Val Loss: 0.1486, Val F1: 0.9406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3216/3216 [01:41<00:00, 31.53it/s, loss=0.0452]\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss: 0.2059, Val Loss: 0.1118, Val F1: 0.9458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3216/3216 [01:42<00:00, 31.49it/s, loss=0.2826]\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss: 0.1963, Val Loss: 0.0998, Val F1: 0.9622\n",
      "Saved best model with F1: 0.9622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3216/3216 [01:42<00:00, 31.33it/s, loss=0.0654]\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Loss: 0.1906, Val Loss: 0.0957, Val F1: 0.9622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3216/3216 [01:42<00:00, 31.43it/s, loss=0.2777]\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Loss: 0.1849, Val Loss: 0.0910, Val F1: 0.9653\n",
      "Saved best model with F1: 0.9653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3216/3216 [01:42<00:00, 31.30it/s, loss=0.0996]\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Loss: 0.1792, Val Loss: 0.0858, Val F1: 0.9636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3216/3216 [01:42<00:00, 31.47it/s, loss=0.2421]\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train Loss: 0.1734, Val Loss: 0.0806, Val F1: 0.9671\n",
      "Saved best model with F1: 0.9671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3216/3216 [01:42<00:00, 31.45it/s, loss=0.0800]\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Train Loss: 0.1696, Val Loss: 0.0805, Val F1: 0.9693\n",
      "Saved best model with F1: 0.9693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3216/3216 [01:42<00:00, 31.36it/s, loss=0.1161]\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Train Loss: 0.1681, Val Loss: 0.0792, Val F1: 0.9659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3216/3216 [01:42<00:00, 31.33it/s, loss=0.6537]\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Train Loss: 0.1617, Val Loss: 0.0757, Val F1: 0.9686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3216/3216 [01:42<00:00, 31.42it/s, loss=0.0990]\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Train Loss: 0.1616, Val Loss: 0.0785, Val F1: 0.9662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3216/3216 [01:42<00:00, 31.33it/s, loss=0.1252]\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Train Loss: 0.1599, Val Loss: 0.0734, Val F1: 0.9702\n",
      "Saved best model with F1: 0.9702\n",
      "\n",
      "--- Evaluating ViT_Small on Test Set (vit_3s_dataset) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0733\n",
      "Test Accuracy: 0.9698\n",
      "Test F1-score: 0.9696\n",
      "Test ROC AUC: 0.9969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: learning_rate ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: test_accuracy ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: test_f1_score ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     test_loss ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_roc_auc ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    train_loss ‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  val_accuracy ‚ñÅ‚ñÅ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val_f1 ‚ñÅ‚ñÅ‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      val_loss ‚ñà‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_roc_auc ‚ñÅ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         epoch 19\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: learning_rate 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: test_accuracy 0.96979\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: test_f1_score 0.96965\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     test_loss 0.07328\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_roc_auc 0.99691\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    train_loss 0.15994\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  val_accuracy 0.97027\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val_f1 0.97024\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      val_loss 0.07339\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_roc_auc 0.99691\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  warmup_phase False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mViT_Small_vit_3s_dataset_040441\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/nekloyh-none/audio-deepfake-detection/runs/docydruq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/nekloyh-none/audio-deepfake-detection\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250609_040441-docydruq/logs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training ViT_Large ===\n",
      "Using device: cuda\n",
      "Configuring ViT_Large model with 7,347,330 parameters...\n",
      "Loading data from: /kaggle/input/vit-3s-dataset/vit_3s_dataset\n",
      "Train samples: 102896\n",
      "Validation samples: 6996\n",
      "Test samples: 14066\n",
      "Using Batch size: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250609_044740-5hq2eoul\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mViT_Large_vit_3s_dataset_044740\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/nekloyh-none/audio-deepfake-detection\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/nekloyh-none/audio-deepfake-detection/runs/5hq2eoul\u001b[0m\n",
      "Epoch 1/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3216/3216 [08:40<00:00,  6.18it/s, loss=0.3738]\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.3577, Val Loss: 0.1940, Val F1: 0.9163\n",
      "Saved best model with F1: 0.9163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3216/3216 [08:40<00:00,  6.18it/s, loss=0.0524]\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 0.2861, Val Loss: 0.1454, Val F1: 0.9371\n",
      "Saved best model with F1: 0.9371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3216/3216 [08:39<00:00,  6.19it/s, loss=0.2782]\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 0.2654, Val Loss: 0.1493, Val F1: 0.9325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3216/3216 [08:37<00:00,  6.22it/s, loss=0.1506]\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 0.2400, Val Loss: 0.1214, Val F1: 0.9493\n",
      "Saved best model with F1: 0.9493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3216/3216 [08:34<00:00,  6.26it/s, loss=0.2821]\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss: 0.2250, Val Loss: 0.1231, Val F1: 0.9522\n",
      "Saved best model with F1: 0.9522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3216/3216 [08:30<00:00,  6.30it/s, loss=0.0994]\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss: 0.2118, Val Loss: 0.1041, Val F1: 0.9609\n",
      "Saved best model with F1: 0.9609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3216/3216 [08:27<00:00,  6.34it/s, loss=0.2316]\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss: 0.1982, Val Loss: 0.1011, Val F1: 0.9558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3216/3216 [08:29<00:00,  6.31it/s, loss=0.0324]\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss: 0.1880, Val Loss: 0.0963, Val F1: 0.9593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3216/3216 [08:27<00:00,  6.34it/s, loss=0.2732]\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss: 0.1790, Val Loss: 0.1121, Val F1: 0.9506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3216/3216 [08:27<00:00,  6.34it/s, loss=0.0460]\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss: 0.1694, Val Loss: 0.0975, Val F1: 0.9602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3216/3216 [08:26<00:00,  6.35it/s, loss=0.0954]\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss: 0.1611, Val Loss: 0.0858, Val F1: 0.9647\n",
      "Saved best model with F1: 0.9647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3216/3216 [08:28<00:00,  6.33it/s, loss=0.3018]\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Loss: 0.1548, Val Loss: 0.0663, Val F1: 0.9733\n",
      "Saved best model with F1: 0.9733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3216/3216 [08:25<00:00,  6.36it/s, loss=0.2208]\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Loss: 0.1484, Val Loss: 0.0732, Val F1: 0.9706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3216/3216 [08:26<00:00,  6.34it/s, loss=0.1989]\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Loss: 0.1395, Val Loss: 0.0633, Val F1: 0.9733\n",
      "Saved best model with F1: 0.9733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3216/3216 [08:26<00:00,  6.35it/s, loss=0.1457]\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train Loss: 0.1338, Val Loss: 0.0569, Val F1: 0.9763\n",
      "Saved best model with F1: 0.9763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3216/3216 [08:26<00:00,  6.35it/s, loss=0.1722]\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Train Loss: 0.1281, Val Loss: 0.0494, Val F1: 0.9806\n",
      "Saved best model with F1: 0.9806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3216/3216 [08:26<00:00,  6.35it/s, loss=0.0465]\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Train Loss: 0.1234, Val Loss: 0.0583, Val F1: 0.9757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3216/3216 [08:25<00:00,  6.36it/s, loss=0.0296]\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Train Loss: 0.1183, Val Loss: 0.0549, Val F1: 0.9770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3216/3216 [08:26<00:00,  6.35it/s, loss=0.1006]\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Train Loss: 0.1156, Val Loss: 0.0479, Val F1: 0.9812\n",
      "Saved best model with F1: 0.9812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3216/3216 [08:26<00:00,  6.35it/s, loss=0.0319]\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Train Loss: 0.1124, Val Loss: 0.0493, Val F1: 0.9794\n",
      "\n",
      "--- Evaluating ViT_Large on Test Set (vit_3s_dataset) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0529\n",
      "Test Accuracy: 0.9775\n",
      "Test F1-score: 0.9776\n",
      "Test ROC AUC: 0.9983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: learning_rate ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: test_accuracy ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: test_f1_score ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     test_loss ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_roc_auc ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    train_loss ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  val_accuracy ‚ñÅ‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val_f1 ‚ñÅ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      val_loss ‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_roc_auc ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         epoch 19\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: learning_rate 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: test_accuracy 0.97753\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: test_f1_score 0.97756\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     test_loss 0.05292\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_roc_auc 0.99835\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    train_loss 0.11238\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  val_accuracy 0.97927\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val_f1 0.97935\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      val_loss 0.0493\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_roc_auc 0.99861\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  warmup_phase False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mViT_Large_vit_3s_dataset_044740\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/nekloyh-none/audio-deepfake-detection/runs/5hq2eoul\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/nekloyh-none/audio-deepfake-detection\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250609_044740-5hq2eoul/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "training_params_small = {\n",
    "    \"model_size\": \"ViT_Small\", \n",
    "    \"epochs\": 20,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_workers\": 4,\n",
    "}\n",
    "\n",
    "training_params_large = {\n",
    "    \"model_size\": \"ViT_Large\", \n",
    "    \"epochs\": 20,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_workers\": 4,\n",
    "}\n",
    "\n",
    "\n",
    "print(\"=== Training ViT_Small ===\")\n",
    "trained_model_small = run_training(training_params_small)\n",
    "\n",
    "print(\"\\n=== Training ViT_Large ===\")\n",
    "trained_model_large = run_training(training_params_large)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7611148,
     "sourceId": 12090521,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 13753.586088,
   "end_time": "2025-06-09T07:41:33.073675",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-09T03:52:19.487587",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
