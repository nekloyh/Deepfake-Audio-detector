{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from pydub import AudioSegment\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, confusion_matrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from transformers import ViTForImageClassification, ViTFeatureExtractor\n",
    "from tqdm import tqdm\n",
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift\n",
    "import wandb\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install librosa soundfile pydub transformers datasets matplotlib seaborn wandb\n",
    "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU Count: {torch.cuda.device_count()}\")\n",
    "print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"deepfake-audio-detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOR_PATH = '/kaggle/input/fake-or-real-dataset'\n",
    "DFADD_PATH = '/kaggle/input/dfadd-dataset'\n",
    "OUTPUT_DIR = '/kaggle/working/preprocessed'\n",
    "MEL_DIR = '/kaggle/working/mel_spectrograms'\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(os.path.join(OUTPUT_DIR, 'real'), exist_ok=True)\n",
    "os.makedirs(os.path.join(OUTPUT_DIR, 'fake'), exist_ok=True)\n",
    "os.makedirs(os.path.join(MEL_DIR, 'real'), exist_ok=True)\n",
    "os.makedirs(os.path.join(MEL_DIR, 'fake'), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. EDA (Exploratory Data Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_samples(dataset_path, real_folder='real', fake_folder='fake'):\n",
    "    real_files = len(os.listdir(os.path.join(dataset_path, real_folder)))\n",
    "    fake_files = len(os.listdir(os.path.join(dataset_path, fake_folder)))\n",
    "    return real_files, fake_files\n",
    "\n",
    "for_real, for_fake = count_samples(FOR_PATH)\n",
    "dfadd_real, dfadd_fake = count_samples(DFADD_PATH)\n",
    "print(f\"FoR: Real={for_real}, Fake={for_fake}\")\n",
    "print(f\"DFADD: Real={dfadd_real}, Fake={dfadd_fake}\")\n",
    "\n",
    "# Visualize class distribution\n",
    "data = {'Dataset': ['FoR', 'FoR', 'DFADD', 'DFADD'],\n",
    "        'Class': ['Real', 'Fake', 'Real', 'Fake'],\n",
    "        'Count': [for_real, for_fake, dfadd_real, dfadd_fake]}\n",
    "df = pd.DataFrame(data)\n",
    "sns.barplot(x='Dataset', y='Count', hue='Class', data=df)\n",
    "plt.title(\"Class Distribution in FoR and DFADD\")\n",
    "plt.savefig('/kaggle/working/class_distribution.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze and visualize sample audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_audio(file_path):\n",
    "    audio, sr = librosa.load(file_path, sr=None)\n",
    "    duration = librosa.get_duration(y=audio, sr=sr)\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    librosa.display.waveshow(audio, sr=sr)\n",
    "    plt.title(\"Waveform\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    S = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=128)\n",
    "    S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "    librosa.display.specshow(S_dB, sr=sr, x_axis='time', y_axis='mel')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(\"Mel-Spectrogram\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/kaggle/working/sample_audio.png')\n",
    "    plt.close()\n",
    "    return sr, duration\n",
    "\n",
    "sample_file = os.path.join(FOR_PATH, 'real', os.listdir(os.path.join(FOR_PATH, 'real'))[0])\n",
    "sr, duration = analyze_audio(sample_file)\n",
    "print(f\"Sample Rate: {sr}, Duration: {duration}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Preprocess Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(input_path, output_path, target_sr=16000, target_duration=3.0):\n",
    "    try:\n",
    "        audio = AudioSegment.from_file(input_path)\n",
    "        audio = audio.set_frame_rate(target_sr).set_channels(1)\n",
    "        target_length = int(target_duration * 1000)\n",
    "        if len(audio) > target_length:\n",
    "            audio = audio[:target_length]\n",
    "        else:\n",
    "            audio = audio + AudioSegment.silent(duration=target_length - len(audio))\n",
    "        audio.export(output_path, format='wav')\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {input_path}: {e}\")\n",
    "\n",
    "for dataset_path, dataset_name in [(FOR_PATH, 'FoR'), (DFADD_PATH, 'DFADD')]:\n",
    "    for class_name in ['real', 'fake']:\n",
    "        input_dir = os.path.join(dataset_path, class_name)\n",
    "        output_dir = os.path.join(OUTPUT_DIR, class_name)\n",
    "        for file in tqdm(os.listdir(input_dir), desc=f\"Processing {dataset_name} {class_name}\"):\n",
    "            preprocess_audio(os.path.join(input_dir, file), os.path.join(output_dir, file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Handle Class Imbalance (Oversampling real class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment = Compose([\n",
    "    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n",
    "    TimeStretch(min_rate=0.8, max_rate=1.2, p=0.5),\n",
    "    PitchShift(min_semitones=-4, max_semitones=4, p=0.5)\n",
    "])\n",
    "\n",
    "def augment_audio(input_path, output_path, augmentations):\n",
    "    audio, sr = librosa.load(input_path, sr=16000)\n",
    "    augmented_audio = augmentations(audio, sample_rate=sr)\n",
    "    sf.write(output_path, augmented_audio, sr) # Corrected sf.write arguments\n",
    "\n",
    "real_files = os.listdir(os.path.join(OUTPUT_DIR, 'real'))\n",
    "target_count = len(os.listdir(os.path.join(OUTPUT_DIR, 'fake')))\n",
    "\n",
    "if len(real_files) < target_count: # Only oversample if needed\n",
    "    print(f\"Oversampling real class from {len(real_files)} to target count {target_count}\")\n",
    "    current_real_count = len(real_files)\n",
    "    # Iterate through real files and augment until target_count is reached\n",
    "    # To avoid infinite loop or excessive augmentation, let's make sure we augment a specific number of times\n",
    "    # based on the difference needed.\n",
    "    num_augmentations_needed = target_count - current_real_count\n",
    "    \n",
    "    # Distribute augmentations among existing real files\n",
    "    # For simplicity, let's loop through real_files and augment each one until the target is met\n",
    "    # A more sophisticated approach would be to calculate how many times each file needs to be augmented\n",
    "    # or to randomly pick files to augment.\n",
    "    aug_idx = 0\n",
    "    while len(os.listdir(os.path.join(OUTPUT_DIR, 'real'))) < target_count:\n",
    "        for file in real_files:\n",
    "            if len(os.listdir(os.path.join(OUTPUT_DIR, 'real'))) >= target_count:\n",
    "                break\n",
    "            input_path = os.path.join(OUTPUT_DIR, 'real', file)\n",
    "            output_path = os.path.join(OUTPUT_DIR, 'real', f\"aug_{aug_idx}_{file}\") # Unique name for augmented file\n",
    "            try:\n",
    "                augment_audio(input_path, output_path, augment)\n",
    "                aug_idx += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error during augmentation of {input_path}: {e}\")\n",
    "else:\n",
    "    print(\"Real class count is already greater than or equal to fake class count. No oversampling needed.\")\n",
    "print(f\"Final Real samples after oversampling: {len(os.listdir(os.path.join(OUTPUT_DIR, 'real')))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Feature Extraction (Mel-Spectrograms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_to_melspectrogram(input_path, output_path, sr=16000, n_mels=128, n_fft=2048, hop_length=512):\n",
    "    audio, sr = librosa.load(input_path, sr=sr)\n",
    "    S = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=n_mels, n_fft=n_fft, hop_length=hop_length)\n",
    "    S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "    np.save(output_path, S_dB)\n",
    "\n",
    "for class_name in ['real', 'fake']:\n",
    "    input_dir = os.path.join(OUTPUT_DIR, class_name)\n",
    "    output_dir = os.path.join(MEL_DIR, class_name)\n",
    "    for file in tqdm(os.listdir(input_dir), desc=f\"Converting {class_name} to Mel\"):\n",
    "        # Ensure the output file name is unique and ends with .npy\n",
    "        base_name, ext = os.path.splitext(file)\n",
    "        if ext.lower() in ['.wav', '.mp3', '.flac']:\n",
    "            audio_to_melspectrogram(os.path.join(input_dir, file), os.path.join(output_dir, f'{base_name}.npy'))\n",
    "        else:\n",
    "            print(f\"Skipping non-audio file: {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Dataset Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_files = [os.path.join(MEL_DIR, 'real', f) for f in os.listdir(os.path.join(MEL_DIR, 'real')) if f.endswith('.npy')]\n",
    "fake_files = [os.path.join(MEL_DIR, 'fake', f) for f in os.listdir(os.path.join(MEL_DIR, 'fake')) if f.endswith('.npy')]\n",
    "\n",
    "all_files = real_files + fake_files\n",
    "labels = [0] * len(real_files) + [1] * len(fake_files)\n",
    "\n",
    "train_files, temp_files, train_labels, temp_labels = train_test_split(\n",
    "    all_files, labels, test_size=0.3, stratify=labels, random_state=42\n",
    ")\n",
    "val_files, test_files, val_labels, test_labels = train_test_split(\n",
    "    temp_files, temp_labels, test_size=0.5, stratify=temp_labels, random_state=42\n",
    ")\n",
    "print(f\"Train: {len(train_files)}, Val: {len(val_files)}, Test: {len(test_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels, transform=None):\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mel = np.load(self.file_paths[idx])\n",
    "        # Ensure mel-spectrogram has 3 channels for ViT, even if it's a single-channel input conceptually.\n",
    "        # ViT expects 3 channels, so we'll stack the single channel 3 times.\n",
    "        if mel.ndim == 2: # Check if it's (n_mels, time_steps)\n",
    "            mel = np.stack([mel, mel, mel], axis=0) # Make it (3, n_mels, time_steps)\n",
    "        elif mel.ndim == 3 and mel.shape[0] == 1: # If it's (1, n_mels, time_steps)\n",
    "            mel = np.concatenate([mel, mel, mel], axis=0) # Make it (3, n_mels, time_steps)\n",
    "            \n",
    "        if self.transform:\n",
    "            mel = self.transform(mel)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return mel, label\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224, 224), antialias=True) # ViT base expects 224x224 input\n",
    "])\n",
    "\n",
    "train_dataset = AudioDataset(train_files, train_labels, transform)\n",
    "val_dataset = AudioDataset(val_files, val_labels, transform)\n",
    "test_dataset = AudioDataset(test_files, test_labels, transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleResNet(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(SimpleResNet, self).__init__()\n",
    "        # CNN input channel should be 3 if we stack 3 times, otherwise 1\n",
    "        # Assuming we keep the 1-channel mel for CNN and convert to 3 for ViT\n",
    "        # Or, we can make this CNN also accept 3 channels to be consistent with the dataset's transform.\n",
    "        # Let's adjust for 3 channels since the transform is set for 3 channels for ViT.\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        # Calculate output size after conv and pooling layers for the FC layer\n",
    "        # For 128x128 input (from Resize), MaxPool2d reduces to 64x64\n",
    "        # If transforms.Resize((224, 224)) is used, then MaxPool2d(2,2) will make it 112x112\n",
    "        # Let's dynamically calculate the size for robustness\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1)) # Global average pooling\n",
    "        self.fc = nn.Linear(128, num_classes) # Corrected based on adaptive pooling\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.avgpool(x) # Apply global average pooling\n",
    "        x = torch.flatten(x, 1) # Flatten for the fully connected layer\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Initialize models\n",
    "cnn_model = SimpleResNet().cuda()\n",
    "cnn_model = nn.DataParallel(cnn_model)\n",
    "\n",
    "# For ViT, ensure the feature extractor is initialized correctly\n",
    "# The ViTForImageClassification automatically handles the feature extraction as part of its forward pass\n",
    "# when you pass raw pixel values (or in our case, preprocessed mel-spectrograms that resemble images).\n",
    "# The transforms.Resize((224, 224)) makes the mel-spectrogram suitable for ViT's input expectation.\n",
    "vit_model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224', num_labels=2, ignore_mismatched_sizes=True)\n",
    "vit_model = nn.DataParallel(vit_model.cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 2.0]).cuda()) # Example: weighting fake class higher\n",
    "scaler = GradScaler()\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in tqdm(loader, desc=\"Training\"):\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            outputs = model(inputs).logits if hasattr(model, 'module') and hasattr(model.module, 'logits') else model(inputs) # Handle ViT output\n",
    "            loss = criterion(outputs, labels)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(loader)\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    preds, true_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(loader, desc=\"Evaluating\"):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            outputs = model(inputs).logits if hasattr(model, 'module') and hasattr(model.module, 'logits') else model(inputs) # Handle ViT output\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            preds.extend(predicted.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Handle case where true_labels or preds might be empty (e.g., if loader is empty)\n",
    "    if len(true_labels) == 0:\n",
    "        return 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, preds, average='binary', zero_division=0)\n",
    "    # ROC AUC requires probability scores, but here we have binary predictions.\n",
    "    # If you want true AUC, you'd need the raw output scores before softmax/argmax.\n",
    "    # For now, using binary predictions for AUC will give a degenerate result (0 or 1).\n",
    "    # If you have raw scores, change `preds` to `torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()`\n",
    "    auc = roc_auc_score(true_labels, preds)\n",
    "    return precision, recall, f1, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    # 'CNN': (cnn_model, torch.optim.Adam(cnn_model.parameters(), lr=1e-4)),\n",
    "    'ViT': (vit_model, torch.optim.Adam(vit_model.parameters(), lr=1e-4))\n",
    "}\n",
    "\n",
    "num_epochs = 10 # Define number of epochs\n",
    "\n",
    "for model_name, (model, optimizer) in models.items():\n",
    "    print(f\"Training {model_name}...\")\n",
    "    best_f1 = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer)\n",
    "        precision, recall, f1, auc = evaluate(model, val_loader)\n",
    "        wandb.log({\n",
    "            f\"{model_name}_epoch\": epoch + 1,\n",
    "            f\"{model_name}_train_loss\": train_loss,\n",
    "            f\"{model_name}_val_precision\": precision,\n",
    "            f\"{model_name}_val_recall\": recall,\n",
    "            f\"{model_name}_val_f1\": f1,\n",
    "            f\"{model_name}_val_auc\": auc\n",
    "        })\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            torch.save(model.state_dict(), f'/kaggle/working/best_{model_name}.pth')\n",
    "            print(f\"Saved best {model_name} model with F1: {f1:.4f}\")\n",
    "        print(f\"{model_name} Epoch {epoch+1}/{num_epochs}: Loss={train_loss:.4f}, Val F1={f1:.4f}\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(f\"\\nEvaluating {model_name} on Test Set...\")\n",
    "    # Load the best model weights before testing\n",
    "    model.load_state_dict(torch.load(f'/kaggle/working/best_{model_name}.pth'))\n",
    "    precision, recall, f1, auc = evaluate(model, test_loader)\n",
    "    print(f\"{model_name} Test: Precision={precision:.4f}, Recall={recall:.4f}, F1={f1:.4f}, AUC={auc:.4f}\")\n",
    "    wandb.log({\n",
    "        f\"{model_name}_test_precision\": precision,\n",
    "        f\"{model_name}_test_recall\": recall,\n",
    "        f\"{model_name}_test_f1\": f1,\n",
    "        f\"{model_name}_test_auc\": auc\n",
    "    })\n",
    "\n",
    "    # Confusion Matrix\n",
    "    preds, true_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            outputs = model(inputs).logits if hasattr(model, 'module') and hasattr(model.module, 'logits') else model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            preds.extend(predicted.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "    if len(true_labels) > 0: # Only plot if there's data\n",
    "        cm = confusion_matrix(true_labels, preds)\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'])\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title(f'{model_name} Confusion Matrix')\n",
    "        plt.savefig(f'/kaggle/working/{model_name}_confusion_matrix.png')\n",
    "        plt.close()\n",
    "        wandb.log({f\"{model_name}_confusion_matrix\": wandb.Image(f'/kaggle/working/{model_name}_confusion_matrix.png')})\n",
    "    else:\n",
    "        print(f\"No data to generate confusion matrix for {model_name} test set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
